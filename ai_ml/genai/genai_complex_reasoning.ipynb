{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204c19ac-ac61-4590-8b56-160c1eadf491",
   "metadata": {},
   "source": [
    "# GenAI Complex Reasoning / Chain of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a576c-050c-4af7-b868-f05cb8077b26",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b7e240-5314-4a40-b52c-10bc37360aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('google.cloud.storage', 'google-cloud-storage'),\n",
    "    ('google.cloud.bigquery', 'google-cloud-bigquery')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824835d1-ea7d-48e9-899e-ac785740988c",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfcee3f-7726-4874-8bf2-068f30d82587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c7a22d-4fe2-48b6-840a-dce4c09488fd",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c01f5cc-2103-475e-9f48-48b8d211ab84",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14e9c98-3f33-45e0-a1ff-12e02adacb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mg-ce-demos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d69f2c-3d6e-402c-8dea-456e618c1d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "\n",
    "# Set the BUCKET name for saving work:\n",
    "BUCKET = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1fe20b-49c9-4f3a-91c0-64956c6dfd72",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa4ae945-304e-4644-a34c-5d1fb9d784e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import io\n",
    "import base64\n",
    "import asyncio\n",
    "import requests\n",
    "import IPython\n",
    "import datetime, time\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import vertexai.vision_models # Imagen Models\n",
    "import vertexai.preview.vision_models\n",
    "import vertexai.language_models # PaLM and Codey Models\n",
    "import vertexai.generative_models # for Gemini Models\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3910b387-15f7-4123-ab3b-6a80e54fbd43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.67.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf89c2c-c01f-4970-bf89-c862cb028bde",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0869ba7c-7fe7-4d65-a325-851ff646d564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "bq = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "bucket = gcs.lookup_bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a8ac3-bbd8-4b92-96b8-4516d687067a",
   "metadata": {},
   "source": [
    "---\n",
    "## Vertex AI Package\n",
    "\n",
    "With the [vertexai](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai) client there are packages for the types of data being interacted with.  There is also a higher package for preview models (not yet in GA).\n",
    "\n",
    "> **NOTE:** In can be helpful to review the API Documentation at it's source in GitHub for up to the moment release information: [github/googleapis/python-aiplatform](https://github.com/googleapis/python-aiplatform/tree/main)\n",
    "\n",
    "Gemini Text and Multimodal Models:\n",
    "- [vertexai.generative_models()](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.generative_models)\n",
    "    - [vertexai.preview.generative_models()](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.preview.generative_models)\n",
    "\n",
    "Language Models (PaLM and Codey Models):\n",
    "- [vertexai.language_models()](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.language_models)\n",
    "    - [vertexai.preview.language_models()](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.preview.language_models)\n",
    "\n",
    "Vision Models (Imagen Models):\n",
    "- [vertexai.vision_models()](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.vision_models)\n",
    "    - [vertexai.preview.vision_models()](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.preview.vision_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905dbf8f-1711-48b7-ba15-2b540c9fe6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemini Models\n",
    "#gemini_text = vertexai.generative_models.GenerativeModel(\"gemini-1.0-pro-002\")\n",
    "#gemini_multimodal = vertexai.generative_models.GenerativeModel(\"gemini-1.0-pro-vision-001\")\n",
    "#gemini15_multimodal = vertexai.generative_models.GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "gemini_flash = vertexai.generative_models.GenerativeModel(\"gemini-1.5-flash-002\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66324a9-3709-4b62-a198-799012a611bd",
   "metadata": {},
   "source": [
    "## Reasoning flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "325ad31b-94d7-4783-ab4b-a031b2425c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_api_call(prompt, max_tokens, is_final_answer=False, custom_client=None):\n",
    "    global client\n",
    "    if custom_client != None:\n",
    "        client = gemini_flash\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if is_final_answer:\n",
    "                response = client.generate_content(prompt)\n",
    "                return response.text\n",
    "            else:\n",
    "                response = client.generate_content(prompt)\n",
    "                return response.text\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                if is_final_answer:\n",
    "                    return {\"title\": \"Error\", \"content\": f\"Failed to generate final answer after 3 attempts. Error: {str(e)}\"}\n",
    "                else:\n",
    "                    return {\"title\": \"Error\", \"content\": f\"Failed to generate step after 3 attempts. Error: {str(e)}\", \"next_action\": \"final_answer\"}\n",
    "            time.sleep(1)  # Wait for 1 second before retrying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50b529-77cb-4ed3-88a0-ef683cf4719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_prompt, custom_client=None):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an expert AI assistant that explains your reasoning step by step. For each step, provide a title that describes what you're doing in that step, along with the content. Decide if you need another step or if you're ready to give the final answer. Respond in JSON format with 'title', 'content', and 'next_action' (either 'continue' or 'final_answer') keys. USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.\n",
    "\n",
    "Example of a valid JSON response:\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Identifying Key Information\",\n",
    "    \"content\": \"To begin solving this problem, we need to carefully examine the given information and identify the crucial elements that will guide our solution process. This involves...\",\n",
    "    \"next_action\": \"continue\"\n",
    "}```\n",
    "\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you! I will now think step by step following my instructions, starting at the beginning after decomposing the problem.\"}\n",
    "    ]\n",
    "    \n",
    "    steps = []\n",
    "    step_count = 1\n",
    "    total_thinking_time = 0\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        step_data = make_api_call(messages, 300, custom_client=custom_client)\n",
    "        end_time = time.time()\n",
    "        thinking_time = end_time - start_time\n",
    "        total_thinking_time += thinking_time\n",
    "        \n",
    "        steps.append((f\"Step {step_count}: {step_data['title']}\", step_data['content'], thinking_time))\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(step_data)})\n",
    "        \n",
    "        if step_data['next_action'] == 'final_answer' or step_count > 25: # Maximum of 25 steps to prevent infinite thinking time. Can be adjusted.\n",
    "            break\n",
    "        \n",
    "        step_count += 1\n",
    "\n",
    "        # Yield after each step for Streamlit to update\n",
    "        yield steps, None  # We're not yielding the total time until the end\n",
    "\n",
    "    # Generate final answer\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Please provide the final answer based solely on your reasoning above. Do not use JSON formatting. Only provide the text response without any titles or preambles. Retain any formatting as instructed by the original prompt, such as exact formatting for free response or multiple choice.\"})\n",
    "    \n",
    "    start_time = time.time()\n",
    "    final_data = make_api_call(messages, 1200, is_final_answer=True, custom_client=custom_client)\n",
    "    end_time = time.time()\n",
    "    thinking_time = end_time - start_time\n",
    "    total_thinking_time += thinking_time\n",
    "    \n",
    "    steps.append((\"Final Answer\", final_data, thinking_time))\n",
    "\n",
    "    yield steps, total_thinking_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0f19f-2f23-41ef-be79-53530feb58a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edaa412-f17c-47f4-87d0-c9e7aa7c7ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What is a mulligan?\",\n",
    "    \"What do you do when you have an unplayable lie in golf?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96adce9f-250c-47a4-b28b-285ecfe168ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = gemini_flash.generate_content(prompts[1])\n",
    "#response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cc782bb-5fb1-4172-baa2-ecc3d8900eec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you have an unplayable lie in golf, you have several options, all resulting in a penalty stroke:\n",
      "\n",
      "1. **Play the ball as it lies:** This is always an option, though rarely the best one if the lie is truly unplayable.  You'll just have to hit it from where it sits.\n",
      "\n",
      "2. **Take relief under Rule 19:** This is the most common approach for unplayable lies. You can take relief in one of the following ways:\n",
      "\n",
      "    * **One club-length relief:**  Drop the ball within one club-length of where the ball lies, not nearer the hole.\n",
      "    * **Two club-lengths relief:** Drop the ball within two club-lengths of where the ball lies, not nearer the hole.  This is only available if dropping within one club length is impossible due to interference from a hazard or other obstruction.\n",
      "    * **Drop from a point nearer the hole:**  You can drop the ball from a spot nearer the hole, losing one stroke.  This is useful if you have no playable lie within a reasonable distance of the original spot, even considering the one or two club-length options.\n",
      "\n",
      "3. **Take relief under other rules:** In rare circumstances, your unplayable lie might be affected by other rules, such as an obstruction that isn't part of the course. You would need to apply the appropriate rule in that case.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "* **Defining \"Unplayable\":**  This is subjective.  If you believe it's impossible to play a reasonable shot from your lie, then it's considered unplayable.\n",
      "\n",
      "* **Penalty:**  Regardless of the method you choose (except playing it as it lies), you incur a one-stroke penalty.\n",
      "\n",
      "* **Free Drop:**  When taking relief, you must ensure that the ball is dropped from waist height within the specified area and doesn't come to rest in a worse location.  You can drop it multiple times if needed to comply.\n",
      "\n",
      "* **Local Rules:** Always check the scorecard or local rules for any specific regulations regarding unplayable lies on that particular course.\n",
      "\n",
      "In short, if your lie is truly unplayable, take a penalty stroke and choose the relief option that gives you the best chance to play your next shot effectively.  Remember to consult the Rules of Golf for a thorough understanding.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f375a5a-0e85-40a5-a011-dc104fbb2229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "When you have an unplayable lie in golf, you have several options, all resulting in a penalty stroke:\n",
       "\n",
       "1. **Play the ball as it lies:** This is always an option, though rarely the best one if the lie is truly unplayable.  You'll just have to hit it from where it sits.\n",
       "\n",
       "2. **Take relief under Rule 19:** This is the most common approach for unplayable lies. You can take relief in one of the following ways:\n",
       "\n",
       "    * **One club-length relief:**  Drop the ball within one club-length of where the ball lies, not nearer the hole.\n",
       "    * **Two club-lengths relief:** Drop the ball within two club-lengths of where the ball lies, not nearer the hole.  This is only available if dropping within one club length is impossible due to interference from a hazard or other obstruction.\n",
       "    * **Drop from a point nearer the hole:**  You can drop the ball from a spot nearer the hole, losing one stroke.  This is useful if you have no playable lie within a reasonable distance of the original spot, even considering the one or two club-length options.\n",
       "\n",
       "3. **Take relief under other rules:** In rare circumstances, your unplayable lie might be affected by other rules, such as an obstruction that isn't part of the course. You would need to apply the appropriate rule in that case.\n",
       "\n",
       "**Important Considerations:**\n",
       "\n",
       "* **Defining \"Unplayable\":**  This is subjective.  If you believe it's impossible to play a reasonable shot from your lie, then it's considered unplayable.\n",
       "\n",
       "* **Penalty:**  Regardless of the method you choose (except playing it as it lies), you incur a one-stroke penalty.\n",
       "\n",
       "* **Free Drop:**  When taking relief, you must ensure that the ball is dropped from waist height within the specified area and doesn't come to rest in a worse location.  You can drop it multiple times if needed to comply.\n",
       "\n",
       "* **Local Rules:** Always check the scorecard or local rules for any specific regulations regarding unplayable lies on that particular course.\n",
       "\n",
       "In short, if your lie is truly unplayable, take a penalty stroke and choose the relief option that gives you the best chance to play your next shot effectively.  Remember to consult the Rules of Golf for a thorough understanding.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2526649-f870-48f0-8175-a51eab2f2a27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Streaming Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30b13cdd-cd09-43a0-8a01-89c1cea60d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There\n",
      " are a few options available when you find yourself with an unplayable lie in golf:\n",
      "\n",
      "\n",
      "**Take an unplayable lie penalty:** This is the most common option. You will\n",
      " add one stroke to your score and drop the ball in a designated relief area. The relief area is typically two club-lengths from the spot where the unplayable lie occurred\n",
      ", no closer to the hole. You must drop the ball within the relief area, not closer to the hole than the original position.\n",
      "\n",
      "**Hit a provisional\n",
      " ball:** Before deciding the lie is unplayable, you can hit a provisional ball from a different location (often the fairway or rough) where you have a playable lie. If you find the original ball and confirm the lie is unplayable, you can\n",
      " take a one-stroke penalty and play the provisional ball. If you find the original ball and it's playable, you can continue playing it and discard the provisional ball.\n",
      "\n",
      "**Declare the ball lost:** If you are unable to find the ball or\n",
      " believe it is lost, you can declare it lost and proceed under the lost ball rule. This means adding a penalty stroke and hitting another ball from the location of your previous stroke.\n",
      "\n",
      "Here are some additional details about each option:\n",
      "\n",
      "**Unplayable lie penalty:**\n",
      "\n",
      "* This is the simplest and fastest option.\n",
      "\n",
      "* It is the best option if you are not sure if the lie is truly unplayable.\n",
      "* It is the best option if you are in a hurry or don't have time to look for the ball.\n",
      "\n",
      "**Hit a provisional ball:**\n",
      "\n",
      "* This is a good option if you think you might be able to\n",
      " play the original ball.\n",
      "* It is important to hit the provisional ball from a location that is as close as possible to the original ball.\n",
      "* If you find the original ball and it is playable, you must continue playing it.\n",
      "\n",
      "**Declare the ball lost:**\n",
      "\n",
      "* This is a good option if you\n",
      " are sure that the ball is lost.\n",
      "* It is the best option if you cannot find the ball after three minutes of searching.\n",
      "* It is the best option if you are out of bounds or in a water hazard.\n",
      "\n",
      "Ultimately, the best option for dealing with an unplayable lie depends on the specific situation.\n",
      " Consider the location of the ball, the type of lie, your confidence in playing the lie, and the time available to make a decision.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for response in gemini_text.generate_content(prompts[1], stream = True):\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca397f3-cf12-4fb2-abaa-0eac4b3986dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Async Response\n",
    "\n",
    "The client has built in method for awaitable responses that make it easy to make asynchronous request.\n",
    "\n",
    "> For detailed coverage and examples of asynchronous call to these API's, scaling, error handling, and managing fail over regions check out this notebook in the same folder: [Python Asynchronous API Calls](./Python%20Asynchronous%20API%20Calls.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "366e13a4-958f-4d53-971b-216d0a8eafc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Unplayable Lies in Golf\n",
       "\n",
       "An unplayable lie in golf can be frustrating, but don't worry, there are options! Here's what you can do:\n",
       "\n",
       "**1. Take a Penalty Stroke:**\n",
       "\n",
       "This is the simplest option. You can take a one-stroke penalty and drop the ball within two club lengths of the original spot, no closer to the hole. Keep in mind that these two club lengths should be measured in any direction, except towards the hole.\n",
       "\n",
       "**2. Play the Ball as it Lies:**\n",
       "\n",
       "While challenging, you can attempt to play the ball as it lies. This could be a good option if you're confident in your ability to hit a difficult shot or if the penalty stroke would put you in an even worse position.\n",
       "\n",
       "**3. Take Lateral Relief:**\n",
       "\n",
       "If your ball is in a penalty area (water hazard or out-of-bounds), you can take lateral relief. This means dropping the ball within two club lengths of the point where the ball crossed the margin of the penalty area, but not closer to the hole. There is a two-stroke penalty for taking lateral relief.\n",
       "\n",
       "**4. Provisional Ball:**\n",
       "\n",
       "Before hitting your original shot, you can hit a provisional ball in case your original shot ends up being unplayable. You can then play the provisional ball without penalty if your original shot is deemed unplayable.\n",
       "\n",
       "**Additional Considerations:**\n",
       "\n",
       "* Always check the local rules before determining how to proceed with an unplayable lie.\n",
       "* Consider the conditions and your own skill level when choosing the best option for your situation.\n",
       "* Remember to inform your playing partners and marker when taking relief or playing a provisional ball.\n",
       "\n",
       "Here are some helpful resources:\n",
       "\n",
       "* **USGA Rule 19:** https://www.usga.org/content/dam/usga/pdf/2023/rule-books/rules-of-golf.pdf\n",
       "* **R&A Rule 19:** https://www.randa.org/en/rules-and-amateur-status/rules-of-golf\n",
       "* **PGA Instruction on Unplayable Lie:** https://www.pga.com/practice/golf-tips/how-to-play-an-unplayable-lie\n",
       "\n",
       "I hope this helps! Let me know if you have any other questions about golf or anything else."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = await gemini_text.generate_content_async(prompts[1])\n",
    "#print(response.text)\n",
    "IPython.display.Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f19073-b734-413d-9086-62eb3f7d8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_system = \"\"\"\n",
    "Begin by enclosing all thoughts within <thinking> tags, exploring multiple angles and approaches.\n",
    "Break down the solution into clear steps within <step> tags. Start with a 20-step budget, requesting more for complex problems if needed.\n",
    "Use <count> tags after each step to show the remaining budget. Stop when reaching 0.\n",
    "Continuously adjust your reasoning based on intermediate results and reflections, adapting your strategy as you progress.\n",
    "Regularly evaluate progress using <reflection> tags. Be critical and honest about your reasoning process.\n",
    "Assign a quality score between 0.0 and 1.0 using <reward> tags after each reflection. Use this to guide your approach:\n",
    "\n",
    "0.8+: Continue current approach\n",
    "0.5-0.7: Consider minor adjustments\n",
    "Below 0.5: Seriously consider backtracking and trying a different approach\n",
    "\n",
    "\n",
    "If unsure or if reward score is low, backtrack and try a different approach, explaining your decision within <thinking> tags.\n",
    "For mathematical problems, show all work explicitly using LaTeX for formal notation and provide detailed proofs.\n",
    "Explore multiple solutions individually if possible, comparing approaches in reflections.\n",
    "Use thoughts as a scratchpad, writing out all calculations and reasoning explicitly.\n",
    "Synthesize the final answer within <answer> tags, providing a clear, concise summary.\n",
    "Conclude with a final reflection on the overall solution, discussing effectiveness, challenges, and solutions. Assign a final reward score.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777082c-9b1e-47b0-97c1-86025b72e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_call(messages, max_tokens, is_final_answer=False, custom_client=None):\n",
    "    global client\n",
    "    if custom_client != None:\n",
    "        client = custom_client\n",
    "    \n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            if is_final_answer:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"llama-3.1-70b-versatile\",\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.2,\n",
    "            ) \n",
    "                return response.choices[0].message.content\n",
    "            else:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"llama-3.1-70b-versatile\",\n",
    "                    messages=messages,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=0.2,\n",
    "                    response_format={\"type\": \"json_object\"}\n",
    "                )\n",
    "                return json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            if attempt == 2:\n",
    "                if is_final_answer:\n",
    "                    return {\"title\": \"Error\", \"content\": f\"Failed to generate final answer after 3 attempts. Error: {str(e)}\"}\n",
    "                else:\n",
    "                    return {\"title\": \"Error\", \"content\": f\"Failed to generate step after 3 attempts. Error: {str(e)}\", \"next_action\": \"final_answer\"}\n",
    "            time.sleep(1)  # Wait for 1 second before retrying\n",
    "\n",
    "def generate_response(prompt, custom_client=None):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are an expert AI assistant that explains your reasoning step by step. For each step, provide a title that describes what you're doing in that step, along with the content. Decide if you need another step or if you're ready to give the final answer. Respond in JSON format with 'title', 'content', and 'next_action' (either 'continue' or 'final_answer') keys. USE AS MANY REASONING STEPS AS POSSIBLE. AT LEAST 3. BE AWARE OF YOUR LIMITATIONS AS AN LLM AND WHAT YOU CAN AND CANNOT DO. IN YOUR REASONING, INCLUDE EXPLORATION OF ALTERNATIVE ANSWERS. CONSIDER YOU MAY BE WRONG, AND IF YOU ARE WRONG IN YOUR REASONING, WHERE IT WOULD BE. FULLY TEST ALL OTHER POSSIBILITIES. YOU CAN BE WRONG. WHEN YOU SAY YOU ARE RE-EXAMINING, ACTUALLY RE-EXAMINE, AND USE ANOTHER APPROACH TO DO SO. DO NOT JUST SAY YOU ARE RE-EXAMINING. USE AT LEAST 3 METHODS TO DERIVE THE ANSWER. USE BEST PRACTICES.\n",
    "\n",
    "Example of a valid JSON response:\n",
    "```json\n",
    "{\n",
    "    \"title\": \"Identifying Key Information\",\n",
    "    \"content\": \"To begin solving this problem, we need to carefully examine the given information and identify the crucial elements that will guide our solution process. This involves...\",\n",
    "    \"next_action\": \"continue\"\n",
    "}```\n",
    "\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": \"Thank you! I will now think step by step following my instructions, starting at the beginning after decomposing the problem.\"}\n",
    "    ]\n",
    "    \n",
    "    steps = []\n",
    "    step_count = 1\n",
    "    total_thinking_time = 0\n",
    "    \n",
    "    while True:\n",
    "        start_time = time.time()\n",
    "        step_data = make_api_call(messages, 300, custom_client=custom_client)\n",
    "        end_time = time.time()\n",
    "        thinking_time = end_time - start_time\n",
    "        total_thinking_time += thinking_time\n",
    "        \n",
    "        steps.append((f\"Step {step_count}: {step_data['title']}\", step_data['content'], thinking_time))\n",
    "        \n",
    "        messages.append({\"role\": \"assistant\", \"content\": json.dumps(step_data)})\n",
    "        \n",
    "        if step_data['next_action'] == 'final_answer' or step_count > 25: # Maximum of 25 steps to prevent infinite thinking time. Can be adjusted.\n",
    "            break\n",
    "        \n",
    "        step_count += 1\n",
    "\n",
    "        # Yield after each step for Streamlit to update\n",
    "        yield steps, None  # We're not yielding the total time until the end\n",
    "\n",
    "    # Generate final answer\n",
    "    messages.append({\"role\": \"user\", \"content\": \"Please provide the final answer based solely on your reasoning above. Do not use JSON formatting. Only provide the text response without any titles or preambles. Retain any formatting as instructed by the original prompt, such as exact formatting for free response or multiple choice.\"})\n",
    "    \n",
    "    start_time = time.time()\n",
    "    final_data = make_api_call(messages, 1200, is_final_answer=True, custom_client=custom_client)\n",
    "    end_time = time.time()\n",
    "    thinking_time = end_time - start_time\n",
    "    total_thinking_time += thinking_time\n",
    "    \n",
    "    steps.append((\"Final Answer\", final_data, thinking_time))\n",
    "\n",
    "    yield steps, total_thinking_time"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
