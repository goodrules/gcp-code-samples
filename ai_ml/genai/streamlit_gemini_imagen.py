import streamlit as st
from google import genai # new unified SDK
from google.genai import types
import tempfile
import os
import mimetypes
from typing import Iterator
import time

PROJECT_ID = os.environ.get("PROJECT_ID")
LOCATION = "us-central1"

# Use this if using GCP - Vertex
from google.oauth2 import service_account
import os

credentials = service_account.Credentials.from_service_account_file(
    os.environ['GOOGLE_APPLICATION_CREDENTIALS'],
    scopes=['https://www.googleapis.com/auth/cloud-platform']
)

google_genai_client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION, credentials=credentials)

def stream_generate(
    query: str, 
    system_context: str,
    model="gemini-2.0-flash-001",
) -> Iterator[str]:
    """Stream chat responses from Gemini."""
    generate_content_config = types.GenerateContentConfig(
        temperature = 1,
        top_p = 0.95,
        max_output_tokens = 4096,
        response_modalities = ["TEXT"],
        #response_mime_type="application/json",
        system_instruction = system_context,
        safety_settings = [types.SafetySetting(
            category="HARM_CATEGORY_HATE_SPEECH",
            threshold="OFF"
        ),types.SafetySetting(
            category="HARM_CATEGORY_DANGEROUS_CONTENT",
            threshold="OFF"
        ),types.SafetySetting(
            category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
            threshold="OFF"
        ),types.SafetySetting(
            category="HARM_CATEGORY_HARASSMENT",
            threshold="OFF"
        )]
    )

    prompt = query
    
    contents = [
        types.Content(
            role="user",
            parts=[types.Part.from_text(text=prompt)]
        )
    ]
    
    # Get streaming response
    response = google_genai_client.models.generate_content_stream(model=model, contents=contents, config=generate_content_config)
    
    for chunk in response:
        if hasattr(chunk, "text"):
            yield chunk.text

def retry_gemini(prompt, model_name, generation_config, retries = 3):
    for retry in range(retries):
        while True:
            try:
                response = google_genai_client.models.generate_content(
                    model=model_name, 
                    contents=prompt,
                    config = generation_config
                )
            except Exception as e:
                print(f"Attempt {retry + 1} failed: {e}")
                time.sleep(0.5 * 2 ** retry)
#                continue
            break
    return response.text

def retry_imagen(prompt, model_name, retries = 3):
    for retry in range(retries):
        while True:
            try:
                response_image = google_genai_client.models.generate_images(
                    model=model_name,
                    prompt=prompt,
                    config=types.GenerateImagesConfig(
                        number_of_images=1,
                        include_rai_reason=True,
                        output_mime_type='image/jpeg',
                    )
                )
            except Exception as e:
                print(f"Attempt {retry + 1} failed: {e}")
                time.sleep(0.5 * 2 ** retry)
#                continue
            break
    return response_image

st.title("Gemini Bot")

# Add clear history button in the sidebar
with st.sidebar:
    model_name = st.selectbox(
        "Select Gemini Model",
        ["gemini-2.0-flash-001", "gemini-1.5-pro", "gemini-1.5-flash"],
        index=0
    )
    image_model_name = st.selectbox(
        "Select Imagen Model",
        ["imagen-3.0-generate-002"],
        index=0
    )
    system_prompt = st.text_area("System Prompt (optional)")
    if st.button("Clear"):
        #st.session_state.chat_history = []
        st.session_state["user_input"] = ""
        st.rerun()


MODEL = model_name
IMAGE_MODEL = image_model_name

full_response = ""
image_list = []

# Chat input
user_input = st.text_input("Hi! How can I help?", key="input1")

if st.button("Generate Text Response"):
    # Get streaming response
    message_placeholder = st.empty()
            
    for chunk in stream_generate(user_input, system_prompt, model=MODEL):
        full_response += chunk
        message_placeholder.markdown(full_response + "â–Œ")
        time.sleep(0.05)
    
    # Update final response
    message_placeholder.markdown(full_response)

if st.button("Generate Image Response"):         
    print(user_input)
    with st.status("Generating image..."):
        st.write("Validating prompt.")
        st.write("Digging through the digital junk drawer of cool images... Found it!")
        response_image = retry_imagen(user_input, IMAGE_MODEL)
        st.write("Slapping it together.")
        st.image(response_image.generated_images[0].image.image_bytes, caption=["Generated by Imagen 3"])
        st.write("Image deployed.")

        