{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and summarize long-form audio using STT and GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img alt=\"Conceptual Flow\" src=\"slides/Screenshot 2023-08-24 at 12.33.59 PM.png\" width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK, langchain and dependencies\n",
    "!pip install google-cloud-aiplatform vertexai langchain chromadb pydantic typing-inspect typing_extensions pandas datasets google-api-python-client pypdf faiss-cpu transformers config --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "#import IPython\n",
    "\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-14 11:14:30.426238: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI version: 1.32.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, TextEmbeddingModel\n",
    "from vertexai.preview.language_models import TextGenerationModel as TextGenerationModel_preview\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "print(\"Vertex AI version: \" + str(aiplatform.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Env variables and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mg-ce-demos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "LOCATION = 'us'\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "BLOB_PATH = 'audio_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'stt_genai'\n",
    "TRANSCRIPT_TABLE_V1 = 'transcript_stt_genai_demo_v1'\n",
    "TRANSCRIPT_TABLE_CHIRP = 'transcript_stt_genai_demo_chirp'\n",
    "SUMMARY_TABLE = 'summary_stt_genai_demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcs client\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "\n",
    "# vertex ai clients\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "# bigquery client\n",
    "bq = bigquery.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get audio data from GCS\n",
    "def get_audio_data(gcs_bucket, blob_path):\n",
    "    bucket = gcs.bucket(gcs_bucket)\n",
    "\n",
    "    # Get the list of blobs\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    # Loop through the blobs\n",
    "    audio_data = []\n",
    "    for blob in blobs:\n",
    "        if blob.name.startswith(blob_path):\n",
    "            if blob.name.endswith('.mp3'):\n",
    "                #print(blob.name)\n",
    "                audio_data.append([blob.name, blob.content_type, f'gs://{GCS_BUCKET}/{blob.name}'])\n",
    "                \n",
    "    return {\n",
    "        'data_name': audio_data[-1][0],\n",
    "        'data_type': audio_data[-1][1],\n",
    "        'data_uri': audio_data[-1][2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STT using cloud speech v2\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud.speech_v2 import SpeechClient as SpeechClient_v2\n",
    "from google.cloud.speech_v2.types import cloud_speech as cloud_speech_v2\n",
    "\n",
    "def transcribe_gcs_v2(gcs_uri: str) -> str:\n",
    "    TIMEOUT_DEFAULT = 3600\n",
    "\n",
    "    # Instantiates a client\n",
    "    client = SpeechClient_v2(\n",
    "        client_options=ClientOptions(\n",
    "            api_endpoint=\"us-central1-speech.googleapis.com\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    config = cloud_speech_v2.RecognitionConfig(\n",
    "        auto_decoding_config=cloud_speech_v2.AutoDetectDecodingConfig(),\n",
    "        language_codes=[\"en-US\"],\n",
    "        model=\"chirp\",\n",
    "    )\n",
    "    \n",
    "    file_metadata = cloud_speech_v2.BatchRecognizeFileMetadata(uri=gcs_uri)\n",
    "\n",
    "    request = cloud_speech_v2.BatchRecognizeRequest(\n",
    "        recognizer=f\"projects/{PROJECT_ID}/locations/{REGION}/recognizers/chirp-recognizer\",\n",
    "        config=config,\n",
    "        files=[file_metadata],\n",
    "        recognition_output_config=cloud_speech_v2.RecognitionOutputConfig(\n",
    "            inline_response_config=cloud_speech_v2.InlineOutputConfig(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Transcribes the audio into text\n",
    "    operation = client.batch_recognize(request=request)\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    try:\n",
    "        response = operation.result(timeout=TIMEOUT_DEFAULT)       # The default is 3600 seconds, 1 hour\n",
    "\n",
    "    except:\n",
    "        response = operation.result(timeout=1.5 * TIMEOUT_DEFAULT) # 5400 seconds, 1 hour and 30 minutes\n",
    "\n",
    "    finally:\n",
    "        response = operation.result(timeout=2 * TIMEOUT_DEFAULT)   # 7200 seconds, 2 hours\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentences and chunks for prompting\n",
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "    # Combine the non-sentences together\n",
    "    sentences = []\n",
    "\n",
    "    is_new_sentence = True\n",
    "    sentence_length = 0\n",
    "    sentence_num = 0\n",
    "    sentence_segments = []\n",
    "\n",
    "    for i in range(len(segments)):\n",
    "        if is_new_sentence == True:\n",
    "            is_new_sentence = False\n",
    "        # Append the segment\n",
    "        sentence_segments.append(segments[i])\n",
    "        segment_words = segments[i].split(' ')\n",
    "        sentence_length += len(segment_words)\n",
    "\n",
    "        # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "        # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "        if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "            sentence = ' '.join(sentence_segments)\n",
    "            sentences.append({\n",
    "            'sentence_num': sentence_num,\n",
    "            'text': sentence,\n",
    "            'sentence_length': sentence_length\n",
    "            })\n",
    "            # Reset\n",
    "            is_new_sentence = True\n",
    "            sentence_length = 0\n",
    "            sentence_segments = []\n",
    "            sentence_num += 1\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "    sentences_df = pd.DataFrame(sentences)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "        chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "        chunk_text = ' '.join(chunk['text'].tolist())\n",
    "\n",
    "        chunks.append({\n",
    "            'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "            'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "            'text': chunk_text,\n",
    "            'num_words': len(chunk_text.split(' '))\n",
    "        })\n",
    "    \n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    return chunks_df.to_dict('records')\n",
    "\n",
    "def parse_title_summary_results(results):\n",
    "    out = []\n",
    "    for e in results:\n",
    "        e = e.replace('\\n', '')\n",
    "        if '|' in e:\n",
    "            processed = {'title': e.split('|')[0],\n",
    "                        'summary': e.split('|')[1][1:]\n",
    "                        }\n",
    "        elif ':' in e:\n",
    "            processed = {'title': e.split(':')[0],\n",
    "                        'summary': e.split(':')[1][1:]\n",
    "                        }\n",
    "        elif '-' in e:\n",
    "            processed = {'title': e.split('-')[0],\n",
    "                        'summary': e.split('-')[1][1:]\n",
    "                        }\n",
    "        else:\n",
    "            processed = {'title': '',\n",
    "                        'summary': e\n",
    "                        }\n",
    "        out.append(processed)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GenAI summary stage 1 using raw text chunks\n",
    "def summary_stage_1(chunks_text):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prompt to get title and summary for each chunk\n",
    "    map_prompt_template = \"\"\"Summarize the key points of the following text. Include as much information as possible:\n",
    "\n",
    "    {text}\"\"\"\n",
    "\n",
    "    map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = VertexAI(model_name=\"text-bison-32k\",\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        verbose=True,)\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "    map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "    stage_1_outputs_summary = [e['text'] for e in map_llm_chain_results]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'Summary creation - Stage 1: {total_time} minutes')\n",
    "\n",
    "    return {\n",
    "        'stage_1_outputs_summary': stage_1_outputs_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm to determine distinct topics\n",
    "def get_topics(stage_1_summaries, num_1_chunks, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "    # Use Vertex AI to embed the summaries and titles. Size of _embeds: (num_chunks x 768)\n",
    "    vertex_embed = VertexAIEmbeddings()\n",
    "\n",
    "    summary_embeds = np.array(vertex_embed.embed_documents(stage_1_summaries))\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "        for col in range(row, num_1_chunks):\n",
    "            # Calculate cosine similarity between the two vectors\n",
    "            similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "            summary_similarity_matrix[row, col] = similarity\n",
    "            summary_similarity_matrix[col, row] = similarity\n",
    "    \n",
    "    # Draw a heatmap with the summary_similarity_matrix\n",
    "    plt.figure()\n",
    "    # Color scheme blues\n",
    "    plt.imshow(summary_similarity_matrix, cmap = 'Blues')\n",
    "    \n",
    "    title_similarity = summary_similarity_matrix\n",
    "\n",
    "    proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "    for row in range(proximity_bonus_arr.shape[0]):\n",
    "        for col in range(proximity_bonus_arr.shape[1]):\n",
    "            if row == col:\n",
    "                proximity_bonus_arr[row, col] = 0\n",
    "            else:\n",
    "                proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "    title_similarity += proximity_bonus_arr\n",
    "\n",
    "    title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "    desired_num_topics = num_topics\n",
    "    # Store the accepted partitionings\n",
    "    topics_title_accepted = []\n",
    "\n",
    "    resolution = 0.85\n",
    "    resolution_step = 0.01\n",
    "    iterations = 40\n",
    "\n",
    "    # Find the resolution that gives the desired number of topics\n",
    "    topics_title = []\n",
    "    while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "        topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "        resolution += resolution_step\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "    lowest_sd_iteration = 0\n",
    "    # Set lowest sd to inf\n",
    "    lowest_sd = float('inf')\n",
    "\n",
    "    for i in range(iterations):\n",
    "        topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "        modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "        # Check SD\n",
    "        topic_sizes = [len(c) for c in topics_title]\n",
    "        sizes_sd = np.std(topic_sizes)\n",
    "\n",
    "        topics_title_accepted.append(topics_title)\n",
    "\n",
    "        if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "            lowest_sd_iteration = i\n",
    "            lowest_sd = sizes_sd\n",
    "      \n",
    "    # Set the chosen partitioning to be the one with highest modularity\n",
    "    topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "    print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}, Number of topics (actual): {len(topics_title)}, Number of topics (setting): {desired_num_topics}')\n",
    "\n",
    "    topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "    # Arrange title_topics in order of topic_id_means\n",
    "    topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "    # Create an array denoting which topic each chunk belongs to\n",
    "    chunk_topics = [None] * title_similarity.shape[0]\n",
    "    for i, c in enumerate(topics_title):\n",
    "        for j in c:\n",
    "            chunk_topics[j] = i\n",
    "            \n",
    "    return {\n",
    "        'chunk_topics': chunk_topics,\n",
    "        'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GenAI summary stage 2 referencing topics and creation of articles\n",
    "def summary_stage_2(summary_data, topics, summary_num_words = 250):\n",
    "    \n",
    "    start_time = time.time()\n",
    "  \n",
    "    # Prompt to get title and summary for each chunk\n",
    "    MAP_PROMPT_TEMPLATE = \"\"\"Write a \"\"\" + str(summary_num_words) + \"\"\" word summary of the following text.  Include as much information as possible:\n",
    "  \n",
    "    {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    map_prompt = PromptTemplate(template=MAP_PROMPT_TEMPLATE, input_variables=[\"text\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = VertexAI(model_name=\"text-bison-32k\",\n",
    "        max_output_tokens=512,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        verbose=True,)\n",
    "  \n",
    "    topics_data = []\n",
    "    for c in topics:\n",
    "        topic_data_temp = [stage_1_summaries[chunk_id] for chunk_id in c]\n",
    "        topic_data_temp = '. '.join(topic_data_temp)\n",
    "        topics_data.append(topic_data_temp)\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "    map_llm_chain_input = [{'text': t} for t in topics_data]\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "    stage_2_outputs_summary = [e['text'] for e in map_llm_chain_results]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'Summary creation - Stage 2: {total_time} minutes')\n",
    "\n",
    "    return {\n",
    "        'stage_2_outputs_summary': stage_2_outputs_summary\n",
    "    }\n",
    "\n",
    "def title_stage_2(summary_data, topics, summary_num_words = 250):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prompt to get title and summary for each chunk\n",
    "    MAP_PROMPT_TEMPLATE = \"\"\"Write a short title of the following text.  Do not include an explanation:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    map_prompt = PromptTemplate(template=MAP_PROMPT_TEMPLATE, input_variables=[\"text\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = VertexAI(model_name=\"text-bison-32k\",\n",
    "        max_output_tokens=256,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        verbose=True,)\n",
    "        \n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "    map_llm_chain_input = [{'text': t} for t in summary_data]\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "    stage_2_outputs_title = [e['text'] for e in map_llm_chain_results]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'Title creation - Stage 2: {total_time} minutes')\n",
    "\n",
    "    return {\n",
    "        'stage_2_outputs_title': stage_2_outputs_title\n",
    "    }\n",
    "\n",
    "def article_stage_2(stage_1_summaries, num_drafts=1):\n",
    "    # set number of drafts to generate\n",
    "    NUM_DRAFTS = num_drafts\n",
    "\n",
    "    # Query Palm 2 to get a summary and title for each topic\n",
    "    drafts = []\n",
    "    for i in range(0, NUM_DRAFTS):\n",
    "        print(f\"Draft {i}\")\n",
    "        stage_2_outputs_summary = summary_stage_2(stage_1_summaries, topics, summary_num_words = 250)['stage_2_outputs_summary']\n",
    "        stage_2_outputs_title = title_stage_2(stage_2_outputs_summary, topics, summary_num_words = 250)['stage_2_outputs_title']\n",
    "        drafts.append([i+1, stage_2_outputs_title, stage_2_outputs_summary])\n",
    "    \n",
    "    return {\n",
    "        'drafts': drafts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data: Get data from GCS\n",
    "\n",
    "Can loop through GCS to pick up multiple files.  This iteration is built for one file at a time but easy to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through GCS bucket(s) for audio files to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mg-ce-demos/audio_data/test/Mapping_uncharted_undersea_volcanoes,_and_elephant_seals_dive_deep_to_sleep.mp3\n"
     ]
    }
   ],
   "source": [
    "sample_uri = get_audio_data(GCS_BUCKET, BLOB_PATH)['data_uri']\n",
    "print(sample_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data: Transcribe audio and stringify\n",
    "\n",
    "Until we can associate specific comments with specific people (e.g. using something like Speaker ID), speaker diarization doesn't add much value.  In v2, this was removed so we could test with the new Chirp model which did appear to improve accuracy of the transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transcribe audio, including speaker diarization and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mg-ce-demos.stt_genai.transcript_stt_genai_demo_chirp already exists.\n",
      "The table is not empty\n"
     ]
    }
   ],
   "source": [
    "## BQ table check\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{DATASET}.{TRANSCRIPT_TABLE_CHIRP}\"\n",
    "\n",
    "prior_transcription=False\n",
    "try:\n",
    "    bq.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    if bq.query(f\"SELECT COUNT(*) FROM {table_id}\").result().total_rows == 0:\n",
    "        print(\"The table is empty\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = transcribe_gcs_v2(sample_uri)\n",
    "        \n",
    "        order = 0\n",
    "\n",
    "        transcript_df2 = pd.DataFrame(columns=['file', 'order', 'text'])\n",
    "\n",
    "        for result in response.results[sample_uri].transcript.results:\n",
    "            transcript_df2.loc[len(transcript_df2.index)] = [sample_uri, order, result.alternatives[0].transcript]\n",
    "            order += 1\n",
    "\n",
    "        transcript_df2.head()\n",
    "        transcript_string = transcript_df2['text'].str.cat(sep=' ')\n",
    "\n",
    "        table_id = f'{PROJECT_ID}.{DATASET}.{TRANSCRIPT_TABLE_CHIRP}'\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "        )\n",
    "\n",
    "        job = bq.load_table_from_dataframe(\n",
    "            transcript_df2, table_id, job_config=job_config\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete.\n",
    "        table = bq.get_table(table_id)  # Make an API request.\n",
    "        print(\n",
    "            \"Loaded {} rows and {} columns to {}\".format(\n",
    "                table.num_rows, len(table.schema), table_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = (end_time - start_time)/60\n",
    "        print(f'{total_time} minutes')\n",
    "    else:\n",
    "        print(\"The table is not empty\")\n",
    "        prior_transcription=True\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "          *\n",
    "        FROM `{0}`\n",
    "        \"\"\".format(table_id)\n",
    "        transcript_df2 = bq.query(query).to_dataframe()\n",
    "        transcript_df2 = transcript_df2.sort_values(by=['order'])\n",
    "        transcript_string = transcript_df2['text'].str.cat(sep=' ')\n",
    "        transcript_string = transcript_string.replace('a.m.', 'am')\n",
    "        transcript_string = transcript_string.replace('p.m.', 'pm')\n",
    "        transcript_string = transcript_string.replace('.com', ' dot com')\n",
    "\n",
    "except NotFound:\n",
    "    print(\"Table {} is not found.\".format(table_id))\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = transcribe_gcs_v2(sample_uri)\n",
    "    \n",
    "    order = 0\n",
    "\n",
    "    transcript_df2 = pd.DataFrame(columns=['file', 'order', 'text'])\n",
    "\n",
    "    for result in response.results[sample_uri].transcript.results:\n",
    "        transcript_df2.loc[len(transcript_df2.index)] = [sample_uri, order, result.alternatives[0].transcript]\n",
    "        order += 1\n",
    "        \n",
    "    transcript_df2.head()\n",
    "    transcript_string = transcript_df2['text'].str.cat(sep=' ')\n",
    "    transcript_string = transcript_string.replace('a.m.', 'am')\n",
    "    transcript_string = transcript_string.replace('p.m.', 'pm')\n",
    "    transcript_string = transcript_string.replace('.com', ' dot com')\n",
    "    transcript_string = transcript_string.replace('.org', ' dot org')\n",
    "    \n",
    "    table_id = f'{PROJECT_ID}.{DATASET}.{TRANSCRIPT_TABLE_CHIRP}'\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "    )\n",
    "\n",
    "    job = bq.load_table_from_dataframe(\n",
    "        transcript_df2, table_id, job_config=job_config\n",
    "    )\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    table = bq.get_table(table_id)  # Make an API request.\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            table.num_rows, len(table.schema), table_id\n",
    "        )\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'{total_time} minutes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build transcription, by speaker, grabbing words passing the confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35973"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transcript_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 32K Model Summarization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen_model_32k = TextGenerationModel_preview.from_pretrained('text-bison-32k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_string_32k = transcript_string[0:30000]\n",
    "len(transcript_string_32k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " - The ocean is mostly unmapped. Only about 25% of the ocean floor has been charted.\n",
       "- New research nearly doubled the number of known seamounts, underwater volcanoes, from 2500 to over 19,000.\n",
       "- Seamounts are important for understanding the ocean's circulation and the distribution of life in the ocean.\n",
       "- Elephant seals are able to sleep while diving underwater.\n",
       "- They sleep in short periods of about 5 minutes at a time and can sleep for a total of 2-10 hours per day.\n",
       "- While sleeping, they flip upside down and spin in a circle, which may help them to conserve energy and avoid predators.\n",
       "- Addiction is a chronic mental health condition that kills hundreds of thousands of Americans every year.\n",
       "- Researchers define addiction as a compulsive behavior that persists despite clear evidence suggesting that the behavior should stop.\n",
       "- Addiction has a devastating impact on society, including causing premature death, and negatively impacting every aspect of life.\n",
       "- The number of addiction-related deaths has increased dramatically in recent years, with over 100,000 Americans dying each year from drug overdoses.\n",
       "- Factors contributing to the increased instance of addiction include the excessive consumption of opioids and other illicit drugs in the United States.\n",
       "- Advances in the science of addiction in recent years have included a better understanding of how drugs of abuse act on the brain's reward pathways, the role of genetics in addiction, and the development of new medications to treat addiction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 32k model summarization test\n",
    "# Ask the LLM\n",
    "prompt = \"\"\"Summarize the key points of the text below.  Write the summary in bullet form with 1-2 sentences per key point.\n",
    "\n",
    "Text:\n",
    "{}\"\"\".format(transcript_string_32k)\n",
    "\n",
    "# Send prompt to LLM\n",
    "response_32k = textgen_model_32k.predict(\n",
    "   (prompt),\n",
    "    max_output_tokens=2000,\n",
    "    temperature=0.4,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "display(Markdown(str(response_32k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create sentences and chunks\n",
    "\n",
    "After transcription, we need to isolate sentences, and create chunks of content.  Unlikely we will get much meaningful context from individual sentences.  Also, we don't want to lose context through chunking so we also will overlap when we combine sentences (i.e. sentences->chunks 1-2-3-4, 2-3-4-5, 3-4-5-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get segments from txt by splitting on .\n",
    "segments =  transcript_string.split('.')\n",
    "# Put the . back in\n",
    "segments = [segment + '.' for segment in segments]\n",
    "# Further split by comma\n",
    "#segments = [segment.split(',') for segment in segments]\n",
    "# Flatten\n",
    "#segments = [item for sublist in segments for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chunk size and overlap\n",
    "sentences = create_sentences(segments, MIN_WORDS=10, MAX_WORDS=100)\n",
    "chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "chunks_text = [chunk['text'] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks_text))\n",
    "#chunks_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stage 1: Get Chunk Summaries\n",
    "\n",
    "Now we create our first summaries and titles based on the chunks of sentences we created from the transcriptions.  This is done using langchain and chaining prompts to create the summary and subsequently the titles.  These can be accomplished in a single step but I found the outputs not as good as doing them sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary creation - Stage 1: 19.711163187026976 minutes\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 1 Summarizing\n",
    "stage_1_summaries = summary_stage_1(chunks_text)['stage_1_outputs_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor data cleanup\n",
    "stage_1_summaries = [\" \" if x == '' else x for x in stage_1_summaries]\n",
    "stage_1_summaries = [string.replace('\\n', ' ') for string in stage_1_summaries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generate embeddings for titles and summaries, and cluster into distinct topics\n",
    "\n",
    "Using the stage 1 summaries, now we group them into common topics using cosine similarity and Louvain community detection.  This is necessary as the content contains many different topics, which when combining will make it hard for LLMs to produce concise content.  This step in the process produces the final context for creating final summaries / articles for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SD: 0.9270248108869579, Best iteration: 0, Number of topics (actual): 8, Number of topics (setting): 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set num_topics to be 1/4 of the number of chunks, or 12, which ever is smaller\n",
    "num_1_chunks = len(stage_1_summaries)\n",
    "NUM_TOPICS = min(int(num_1_chunks / 4), 12)\n",
    "topics_out = get_topics(stage_1_summaries, num_1_chunks, num_topics = NUM_TOPICS, bonus_constant = 0.3)\n",
    "chunk_topics = topics_out['chunk_topics']\n",
    "topics = topics_out['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6],\n",
       " [8, 9, 7],\n",
       " [10, 11, 12, 13],\n",
       " [16, 17, 14, 15],\n",
       " [18, 19, 20, 21],\n",
       " [32, 0, 31],\n",
       " [24, 25, 22, 23],\n",
       " [26, 27, 28, 29, 30]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a heatmap of this array\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "# Draw vertical black lines for every 1 of the x-axis \n",
    "for i in range(1, len(chunk_topics)):\n",
    "    plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stage 2: Final Summaries\n",
    "\n",
    "Using the topics identified prior to this step, we use the same prompt chaining approach to create a summary then title for each topic.  At this point, we have a **first draft** for each topic where the LLM was able to produce an informative article and title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft 0\n",
      "Summary creation - Stage 2: 5.361589701970418 minutes\n",
      "Title creation - Stage 2: 4.169548483689626 minutes\n"
     ]
    }
   ],
   "source": [
    "drafts = article_stage_2(stage_1_summaries, num_drafts=1)['drafts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DRAFT direct 32K summarization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " - The ocean is mostly unmapped. Only about 25% of the ocean floor has been charted.\n",
       "- New research nearly doubled the number of known seamounts, underwater volcanoes, from 2500 to over 19,000.\n",
       "- Seamounts are important for understanding the ocean's circulation and the distribution of life in the ocean.\n",
       "- Elephant seals are able to sleep while diving underwater.\n",
       "- They sleep in short periods of about 5 minutes at a time and can sleep for a total of 2-10 hours per day.\n",
       "- While sleeping, they flip upside down and spin in a circle, which may help them to conserve energy and avoid predators.\n",
       "- Addiction is a chronic mental health condition that kills hundreds of thousands of Americans every year.\n",
       "- Researchers define addiction as a compulsive behavior that persists despite clear evidence suggesting that the behavior should stop.\n",
       "- Addiction has a devastating impact on society, including causing premature death, and negatively impacting every aspect of life.\n",
       "- The number of addiction-related deaths has increased dramatically in recent years, with over 100,000 Americans dying each year from drug overdoses.\n",
       "- Factors contributing to the increased instance of addiction include the excessive consumption of opioids and other illicit drugs in the United States.\n",
       "- Advances in the science of addiction in recent years have included a better understanding of how drugs of abuse act on the brain's reward pathways, the role of genetics in addiction, and the development of new medications to treat addiction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown('# DRAFT direct 32K summarization'))\n",
    "display(Markdown(str(response_32k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DRAFT 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Discovery of 19,000 New Seamounts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " This week on the podcast, Sarah Cresty discusses the discovery of numerous submarine volcanoes known as seamounts, which nearly doubled the previously known number.  Next, Jessica Kendle Bar  joins the show to talk about her research on the sleep patterns of elephant seals, which spend most of their time in water.  In a sponsored segment, Jackie Oburst interviews addiction researchers Eric Nestler and Paul Kenny about the latest findings and future directions in the field of addiction.  Finally, it is noted that only a small portion of the ocean floor has been mapped or charted.  Only about 25% of the ocean floor has been charted or mapped.  - We don't know where all the big dips like trenches are or the big peaks like seamounts.  - The ocean is mysterious because it's covered in water, which makes it difficult to see what's down there.  - The best method we have for mapping the ocean is sonar, but it's expensive and time-consuming.  - Unmapped areas of the ocean can be dangerous for ships and submarines.  - The best method for mapping the ocean is sonar, but it is expensive and we have only mapped 25% of the ocean with it.    - Seamounts are a hazard for submarines, and there have been several incidents where US Navy submarines have run into uncharted seamounts.    - The new study used radar-equipped satellites to measure the height of the ocean worldwide and detect changes in slope in the water at the ocean surface.    - These changes in slope can indicate the gravity of seamounts underwater, and the study was able to identify over 19,000 new seamounts.    - We previously knew about about 2500 seamounts, and the new study was able to identify many smaller seamounts that we were not previously aware of.  Scientists have discovered 19,000 new seamounts, bringing the total number known to 21,500.  The new seamounts were discovered using improved satellite data.  Seamounts are underwater volcanoes that are at least a kilometer high.  The distribution of the new seamounts is similar to that of the previously known seamounts.  The text discusses the significance of newly discovered seamounts and their implications in various fields. The key points are as follows:   1.  New seamounts have been discovered in unexpected places, including around Antarctica"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Elephant Seal Sleep Patterns"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Northern elephant seals spend most of their time on long foraging dives that go very deep, with only one to two minutes at the surface at a time. They come on land twice a year to breed and molt, and spend a lot of time sleeping during that time, but that's not the most accurate representation of their activity budgets when they're out at sea. Other marine mammals, such as dolphins and seals, can sleep in just half of their brain at a time, which allows them to keep one eye open and monitor predators while they're getting the benefits of sleep for half their brain. \n",
       "\n",
       "Whales, dolphins, and seals can sleep in just half of their brain at a time, which is called unihemispheric sleep. This allows them to keep one eye open and monitor predators while they are getting the benefits of sleep for half of their brain. The first night effect is a documented effect where people are slightly more aware when they go to a new place. This is because there is some evidence that there is a little bit of asymmetry in the degree to which each of the hemispheres are engaging in slow-wave sleep. \n",
       "\n",
       "A new method for detecting underwater methane seeps using satellite observations has been developed. The method was developed by Paul Wennberg and his colleagues at Caltech, and it involves using a satellite to measure the amount of methane in the atmosphere. When methane seeps from the ocean floor, it enters the atmosphere and can be detected by satellites. The new method is more efficient than previous methods, which involved using ships or aircraft to measure methane levels. The new method can also be used to detect methane seeps in remote areas that are difficult to reach by ship or aircraft. \n",
       "\n",
       "A study conducted by Jessica Kendall-Bar and her colleagues at the University of California, Santa Cruz looked at the sleep patterns of elephant seals. Elephant seals spend most of their lives in the water, and they need to come to the surface to breathe. This means that they can't sleep deeply for long periods of time, because they need to wake up to breathe. \n",
       "\n",
       "Kendall-Bar and her colleagues found that elephant seals have a unique sleep pattern. They sleep in short bursts, and they wake up frequently to breathe. This sleep pattern allows them to get the rest they need without being at risk of drowning."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Monitoring the Mysterious Underwater World of Elephant Seals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " The study observed elephant seals, which spend a significant amount of time in deep water and hold their breath for extended periods. To monitor their behavior, sleep monitors and motion sensors were attached to the seals. The data collected provided insights into their diving patterns and movements. The researcher's aim was to gain a better understanding of the seals' underwater experience.\n",
       "\n",
       "The speaker is passionate about creating data-driven animations that show the behavior of seals underwater. They built tools to visualize the seals' movements, including their pitch, roll, and swimming frequency. The animations also show the seals' sleep spirals, which are beautiful patterns that the seals create while sleeping underwater.\n",
       "\n",
       "The speaker also discusses the sleep moderate, which is a device used to measure brain activity during sleep. The same sensors used in human sleep studies are used on the seals. The speaker has even tried out the methods on themselves, floating in shallow water and trying to sleep to see if the sensors were working properly.\n",
       "\n",
       "Elephant seals are large animals and can be dangerous to work with. They are often sedated to ensure the safety of the crew and the animals.\n",
       "\n",
       "Elephant seals were tagged with headcaps that monitored sleep and data loggers that stored the data. The study found that elephant seals sleep in short bursts, with an average sleep time of about 5 minutes. They spend most of their time in a state of quiet rest, which is characterized by slow, regular breathing and low levels of muscle activity.\n",
       "\n",
       "During deep sleep, elephant seals experience periods of rapid eye movement (REM), which is associated with dreaming. REM sleep is characterized by fast, irregular breathing and high levels of muscle activity. The study also found that elephant seals spend more time in deep sleep when they are in deeper water. This suggests that deep sleep may be important for helping elephant seals to conserve energy when they are far from land."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Addiction: A Medical Problem That Needs Medical Solutions"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " The Science Postdoctoral Fellow at the Scripps Institution of Oceanography at UC San Diego is Jessica Kendall Bar. The paper discussed in the podcast is available at science.org/podcast. Jackie Oburst, a custom publishing assistant editor, chats with Eric Nestler and Paul Kenny, two experts on addiction, about the latest research in the field. The podcast is sponsored by the Icon School of Medicine at Mount Sinai.\n",
       "Addiction is a chronic mental health condition that kills hundreds of thousands of Americans every year. It can destroy marriages, friendships, and careers, and threaten a person's basic health and safety. Addiction is a medical problem that needs medical solutions.\n",
       "Two renowned experts on addiction are Dr. Nora Volkow, Director of the National Institute on Drug Abuse, and Dr. Charles O'Brien, Professor of Psychiatry at the University of Pennsylvania. They discuss the latest research on addiction and how it can be treated.\n",
       "Addiction is a complex disease that affects millions of people worldwide. The science of addiction is rapidly evolving, and new insights into the underlying mechanisms of addiction are emerging all the time. Drs. Eric Nessler and Paul Kenny are two of the world's leading experts on addiction. They have co-authored an upcoming article about the science of addiction in a neuroscience supplement of Science. Their laboratory studies the molecular mechanisms of drug addiction and depression in animal models. They are also working to develop new treatments for addiction.\n",
       "Addiction is a psychiatric syndrome that can only be diagnosed by talking to a person or their family members and finding out what types of abnormal behaviors they exhibit. Researchers and clinicians do not define addiction the same way. Dr. Paul Kenny is a Ward Coleman's professor and chair of the Nash Family Department of neuroscience as well as the director of the Drug Discovery Institute at the Ikon School of Medicine at Mount Sini. Dr. Kenny's research involves the study of behavioral paradigms, physiological analyses and the molecular underpinnings of neurobehavioral disorders such as schizophrenia and drug addiction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Science Podcast: The Science of Sound"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " The Science Magazine Podcast is a podcast that covers a wide range of scientific topics.  This episode is about the science of sound.  The podcast is hosted by Erica Burk, director and senior editor of Custom Publishing, and Jackie Oburst. \n",
       "     The guests are Dr. Eric Topol, a cardiologist and geneticist, and Dr. Robert Wachter, a professor of medicine at the University of California, San Francisco.  Dr. Topol discusses how technology is changing the way we diagnose and treat diseases.  Dr. Wachter discusses how the healthcare system is changing and how it needs to change in order to meet the needs of the future.  The podcast is sponsored by the Icahn School of Medicine at Mount Sinai, a leading research medical school in New York City.  Researchers are working to advance understanding of the brain and improve care for disorders like depression, dementia, and drug addiction.  A special supplement to Science magazine, prepared by the Icon School of Medicine in partnership with Science, provides more information.  The supplement can be found on the Science website by searching for \"Frontiers of Medical Research-Brain Science.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Drug Overdose Deaths in the United States"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " The speaker discusses the issue of drug overdose deaths in the United States. They point out that the number of deaths has increased dramatically since the pandemic, with an estimated 100,000 Americans dying each year from drug overdoses. This number is more than the number of American combat troops who died during the Vietnam War. The speaker expresses concern that the United States does not seem to be able to marshal the resources to address this issue. \n",
       "     One of the major reasons for the increased instance of drug overdose deaths is the excessive amounts of opioids that are consumed in the United States. The speaker also points out that other drugs, such as cocaine and methamphetamine, are also contributing to the increase in deaths. \n",
       "     Addiction is a psychiatric syndrome that can only be diagnosed by talking to a person or their family members to find out what types of abnormal behaviors they exhibit. There is no blood test, genetic test, or brain scan that can be used to diagnose addiction. Addiction is a compulsive behavior that persists despite clear evidence suggesting that the behavior should stop, and the behavior is directed towards obtaining a drug of abuse. \n",
       "     Addiction has a devastating impact on society. The economic costs of addiction are estimated to be in the billions of dollars each year. In the United States, an estimated 23.5 million people aged 12 or older had a substance use disorder in 2017. Addiction can lead to a number of health problems, including heart disease, stroke, cancer, liver disease, HIV/AIDS, and mental health problems. Addiction can also lead to social problems, such as unemployment, homelessness, crime, incarceration, child abuse, and neglect. \n",
       "     Addiction has a devastating impact on society. In the United States, the opioid epidemic is a leading cause of premature death. Illicit drug use has negative consequences on every aspect of life and society. Addiction-related deaths are on the rise. Before the pandemic, about 70,000 Americans died every year of a drug overdose, the vast majority of which were unintentional."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Brain Regions, Molecules, and Behavior: Understanding Drug Addiction and Its Treatment"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Reward pathways are ancient parts of the brain that regulate our responses to rewards in the environment. When we take a drug of abuse, our brain experiences it as the most rewarding experience possible, even though it isn't. This corrupts the brain's ability to analyze rewards, and an addicted individual is no longer rewarded by natural things. Instead, they require the sledgehammer effects of a drug of abuse to feel normal. This is why people with addictions are driven so strongly to take drugs. Research has identified brain regions involved in regulating responses to drugs of abuse and the chemical changes they produce to cause addiction. Molecular biology studies focus on changes in gene expression within nerve cells in brain reward regions that drive addiction behavior. Understanding the fundamental actions of drugs of abuse in the brain involves molecular, cellular, circuit levels, and underlying genetics. Medications like buprenorphine and methadone can be effective treatments for opioid addiction. The most promising trend in drug addiction treatment is the advancement of compounds through preclinical development and into human clinical trials. Human experiments are necessary to guide preclinical drug development and increase the likelihood of generating effective therapeutics. Behavioral therapies can be effective interventions for substance use disorders. Surprisingly little is known about the biological mechanisms of action of behavioral therapies. Behavioral therapies combined with medication are currently the most effective way of treating substance use disorders."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(drafts)):\n",
    "    display(Markdown('# DRAFT ' + str(drafts[i][0])))\n",
    "    for j in range(len(drafts[i][1])):\n",
    "        if drafts[i][1][j] != \"\":\n",
    "            if drafts[i][2][j] != \"\":\n",
    "                display(Markdown('### ' + drafts[i][1][j]))\n",
    "                display(Markdown(drafts[i][2][j]))\n",
    "                #print(drafts[i][2][j])\n",
    "                print('------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## UI (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file):\n",
    "    file_path = file.name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    bucket = gcs.bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(f'audio_data/cumulus/{file_name}')\n",
    "    blob.upload_from_filename(f'{file_path}')\n",
    "    #print(\"Uploaded\")\n",
    "    \n",
    "    gcs_path = f'gs://{GCS_BUCKET}/{blob.name}'\n",
    "    \n",
    "    return gcs_path\n",
    "    \n",
    "#with gr.Blocks() as demo:\n",
    "#    file_output = gr.File()\n",
    "#    upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\"audio\"])\n",
    "#    upload_button.upload(upload_file, upload_button, file_output)\n",
    "\n",
    "#demo.launch(share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (754168372.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[32], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# create a function that pulls all the steps together\n",
    "def article_builder(gcs_uri):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_audio_data('mg-ce-demos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "    text = gr.Textbox()\n",
    "    label = gr.Label()\n",
    "\n",
    "    b1 = gr.Button(\"Upload\")\n",
    "    b2 = gr.Button(\"Get GCS URI\")\n",
    "    b3 = gr.Button(\"Create Articles\")\n",
    "\n",
    "    b1.click(upload_file, inputs=audio_file, outputs=text)\n",
    "    b2.click(get_audio_data, inputs=text, outputs=label)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    ## Audio to Articles\n",
    "    \"\"\")\n",
    "    #with gr.Row():\n",
    "    #    file_output = gr.File()\n",
    "    #    upload_button = gr.UploadButton(\"Click to Transcribe and Draft Articles\", file_types=[\"audio\"])\n",
    "    #    upload_button.upload(upload_file, upload_button, file_output)  \n",
    "        \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(label=\"Task\", placeholder=\"Cloud Bucket Name\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        find_gcs_uri = gr.Button(\"Select Audio File\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        label1 = gr.Textbox(label=\"GCS URI\")\n",
    "        \n",
    "    #with gr.Row():\n",
    "    #    generate = gr.Button(\"Generate Response\")\n",
    "\n",
    "    #with gr.Row():\n",
    "    #    label2 = gr.Textbox(label=\"Prompt\")\n",
    "    #with gr.Row():\n",
    "    #    label3 = gr.Textbox(label=\"Response generated by LLM\")\n",
    "\n",
    "    generate.click(get_audio_data, input_text, label1)\n",
    "    \n",
    "demo.launch(share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
