{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and summarize long-form audio using STT and GenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Solution Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img alt=\"Conceptual Flow\" src=\"Screenshot 2023-08-24 at 12.33.59 PM.png\" width=\"100%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Vertex AI LLM SDK, langchain and dependencies\n",
    "!pip install google-cloud-aiplatform vertexai langchain chromadb pydantic typing-inspect typing_extensions pandas datasets google-api-python-client pypdf faiss-cpu transformers config --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "#import IPython\n",
    "\n",
    "#app = IPython.Application.instance()\n",
    "#app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-13 10:43:12.688447: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI version: 1.32.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from networkx.algorithms import community\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, TextEmbeddingModel\n",
    "from vertexai.preview.language_models import TextGenerationModel as TextGenerationModel_preview\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "print(\"Vertex AI version: \" + str(aiplatform.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Env variables and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mg-ce-demos'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "LOCATION = 'us'\n",
    "GCS_BUCKET = PROJECT_ID\n",
    "BLOB_PATH = 'audio_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'stt_genai'\n",
    "TRANSCRIPT_TABLE_V1 = 'transcript_stt_genai_demo_v1'\n",
    "TRANSCRIPT_TABLE_CHIRP = 'transcript_stt_genai_demo_chirp'\n",
    "SUMMARY_TABLE = 'summary_stt_genai_demo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcs client\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "\n",
    "# vertex ai clients\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "# bigquery client\n",
    "bq = bigquery.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get audio data from GCS\n",
    "def get_audio_data(gcs_bucket, blob_path):\n",
    "    bucket = gcs.bucket(gcs_bucket)\n",
    "\n",
    "    # Get the list of blobs\n",
    "    blobs = bucket.list_blobs()\n",
    "\n",
    "    # Loop through the blobs\n",
    "    audio_data = []\n",
    "    for blob in blobs:\n",
    "        if blob.name.startswith(blob_path):\n",
    "            if blob.name.endswith('.mp3'):\n",
    "                #print(blob.name)\n",
    "                audio_data.append([blob.name, blob.content_type, f'gs://{GCS_BUCKET}/{blob.name}'])\n",
    "                \n",
    "    return {\n",
    "        'data_name': audio_data[-1][0],\n",
    "        'data_type': audio_data[-1][1],\n",
    "        'data_uri': audio_data[-1][2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STT using cloud speech v2\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud.speech_v2 import SpeechClient as SpeechClient_v2\n",
    "from google.cloud.speech_v2.types import cloud_speech as cloud_speech_v2\n",
    "\n",
    "def transcribe_gcs_v2(gcs_uri: str) -> str:\n",
    "    TIMEOUT_DEFAULT = 3600\n",
    "\n",
    "    # Instantiates a client\n",
    "    client = SpeechClient_v2(\n",
    "        client_options=ClientOptions(\n",
    "            api_endpoint=\"us-central1-speech.googleapis.com\",\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    config = cloud_speech_v2.RecognitionConfig(\n",
    "        auto_decoding_config=cloud_speech_v2.AutoDetectDecodingConfig(),\n",
    "        language_codes=[\"en-US\"],\n",
    "        model=\"chirp\",\n",
    "    )\n",
    "    \n",
    "    file_metadata = cloud_speech_v2.BatchRecognizeFileMetadata(uri=gcs_uri)\n",
    "\n",
    "    request = cloud_speech_v2.BatchRecognizeRequest(\n",
    "        recognizer=f\"projects/{PROJECT_ID}/locations/{REGION}/recognizers/chirp-recognizer\",\n",
    "        config=config,\n",
    "        files=[file_metadata],\n",
    "        recognition_output_config=cloud_speech_v2.RecognitionOutputConfig(\n",
    "            inline_response_config=cloud_speech_v2.InlineOutputConfig(),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Transcribes the audio into text\n",
    "    operation = client.batch_recognize(request=request)\n",
    "\n",
    "    print(\"Waiting for operation to complete...\")\n",
    "    try:\n",
    "        response = operation.result(timeout=TIMEOUT_DEFAULT)       # The default is 3600 seconds, 1 hour\n",
    "\n",
    "    except:\n",
    "        response = operation.result(timeout=1.5 * TIMEOUT_DEFAULT) # 5400 seconds, 1 hour and 30 minutes\n",
    "\n",
    "    finally:\n",
    "        response = operation.result(timeout=2 * TIMEOUT_DEFAULT)   # 7200 seconds, 2 hours\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sentences and chunks for prompting\n",
    "def create_sentences(segments, MIN_WORDS, MAX_WORDS):\n",
    "\n",
    "    # Combine the non-sentences together\n",
    "    sentences = []\n",
    "\n",
    "    is_new_sentence = True\n",
    "    sentence_length = 0\n",
    "    sentence_num = 0\n",
    "    sentence_segments = []\n",
    "\n",
    "    for i in range(len(segments)):\n",
    "        if is_new_sentence == True:\n",
    "            is_new_sentence = False\n",
    "        # Append the segment\n",
    "        sentence_segments.append(segments[i])\n",
    "        segment_words = segments[i].split(' ')\n",
    "        sentence_length += len(segment_words)\n",
    "\n",
    "        # If exceed MAX_WORDS, then stop at the end of the segment\n",
    "        # Only consider it a sentence if the length is at least MIN_WORDS\n",
    "        if (sentence_length >= MIN_WORDS and segments[i][-1] == '.') or sentence_length >= MAX_WORDS:\n",
    "            sentence = ' '.join(sentence_segments)\n",
    "            sentences.append({\n",
    "            'sentence_num': sentence_num,\n",
    "            'text': sentence,\n",
    "            'sentence_length': sentence_length\n",
    "            })\n",
    "            # Reset\n",
    "            is_new_sentence = True\n",
    "            sentence_length = 0\n",
    "            sentence_segments = []\n",
    "            sentence_num += 1\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def create_chunks(sentences, CHUNK_LENGTH, STRIDE):\n",
    "\n",
    "    sentences_df = pd.DataFrame(sentences)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences_df), (CHUNK_LENGTH - STRIDE)):\n",
    "        chunk = sentences_df.iloc[i:i+CHUNK_LENGTH]\n",
    "        chunk_text = ' '.join(chunk['text'].tolist())\n",
    "\n",
    "        chunks.append({\n",
    "            'start_sentence_num': chunk['sentence_num'].iloc[0],\n",
    "            'end_sentence_num': chunk['sentence_num'].iloc[-1],\n",
    "            'text': chunk_text,\n",
    "            'num_words': len(chunk_text.split(' '))\n",
    "        })\n",
    "    \n",
    "    chunks_df = pd.DataFrame(chunks)\n",
    "    return chunks_df.to_dict('records')\n",
    "\n",
    "def parse_title_summary_results(results):\n",
    "    out = []\n",
    "    for e in results:\n",
    "        e = e.replace('\\n', '')\n",
    "        if '|' in e:\n",
    "            processed = {'title': e.split('|')[0],\n",
    "                        'summary': e.split('|')[1][1:]\n",
    "                        }\n",
    "        elif ':' in e:\n",
    "            processed = {'title': e.split(':')[0],\n",
    "                        'summary': e.split(':')[1][1:]\n",
    "                        }\n",
    "        elif '-' in e:\n",
    "            processed = {'title': e.split('-')[0],\n",
    "                        'summary': e.split('-')[1][1:]\n",
    "                        }\n",
    "        else:\n",
    "            processed = {'title': '',\n",
    "                        'summary': e\n",
    "                        }\n",
    "        out.append(processed)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GenAI summary stage 1 using raw text chunks\n",
    "def summary_stage_1(chunks_text):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prompt to get title and summary for each chunk\n",
    "    map_prompt_template = \"\"\"Summarize the key points of the following text. Include as much information as possible:\n",
    "\n",
    "    {text}\"\"\"\n",
    "\n",
    "    map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = VertexAI(model_name=\"text-bison-32k\",\n",
    "        max_output_tokens=1024,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        verbose=True,)\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "    map_llm_chain_input = [{'text': t} for t in chunks_text]\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "    stage_1_outputs_summary = [e['text'] for e in map_llm_chain_results]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'Summary creation - Stage 1: {total_time} minutes')\n",
    "\n",
    "    return {\n",
    "        'stage_1_outputs_summary': stage_1_outputs_summary\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the community detection algorithm to determine distinct topics\n",
    "def get_topics(stage_1_summaries, num_1_chunks, num_topics = 8, bonus_constant = 0.25, min_size = 3):\n",
    "    # Use Vertex AI to embed the summaries and titles. Size of _embeds: (num_chunks x 768)\n",
    "    vertex_embed = VertexAIEmbeddings()\n",
    "\n",
    "    summary_embeds = np.array(vertex_embed.embed_documents(stage_1_summaries))\n",
    "    \n",
    "    # Get similarity matrix between the embeddings of the chunk summaries\n",
    "    summary_similarity_matrix = np.zeros((num_1_chunks, num_1_chunks))\n",
    "    summary_similarity_matrix[:] = np.nan\n",
    "\n",
    "    for row in range(num_1_chunks):\n",
    "        for col in range(row, num_1_chunks):\n",
    "            # Calculate cosine similarity between the two vectors\n",
    "            similarity = 1- cosine(summary_embeds[row], summary_embeds[col])\n",
    "            summary_similarity_matrix[row, col] = similarity\n",
    "            summary_similarity_matrix[col, row] = similarity\n",
    "    \n",
    "    # Draw a heatmap with the summary_similarity_matrix\n",
    "    plt.figure()\n",
    "    # Color scheme blues\n",
    "    plt.imshow(summary_similarity_matrix, cmap = 'Blues')\n",
    "    \n",
    "    title_similarity = summary_similarity_matrix\n",
    "\n",
    "    proximity_bonus_arr = np.zeros_like(title_similarity)\n",
    "    for row in range(proximity_bonus_arr.shape[0]):\n",
    "        for col in range(proximity_bonus_arr.shape[1]):\n",
    "            if row == col:\n",
    "                proximity_bonus_arr[row, col] = 0\n",
    "            else:\n",
    "                proximity_bonus_arr[row, col] = 1/(abs(row-col)) * bonus_constant\n",
    "        \n",
    "    title_similarity += proximity_bonus_arr\n",
    "\n",
    "    title_nx_graph = nx.from_numpy_array(title_similarity)\n",
    "\n",
    "    desired_num_topics = num_topics\n",
    "    # Store the accepted partitionings\n",
    "    topics_title_accepted = []\n",
    "\n",
    "    resolution = 0.85\n",
    "    resolution_step = 0.01\n",
    "    iterations = 40\n",
    "\n",
    "    # Find the resolution that gives the desired number of topics\n",
    "    topics_title = []\n",
    "    while len(topics_title) not in [desired_num_topics, desired_num_topics + 1, desired_num_topics + 2]:\n",
    "        topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "        resolution += resolution_step\n",
    "    topic_sizes = [len(c) for c in topics_title]\n",
    "    sizes_sd = np.std(topic_sizes)\n",
    "    modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "    lowest_sd_iteration = 0\n",
    "    # Set lowest sd to inf\n",
    "    lowest_sd = float('inf')\n",
    "\n",
    "    for i in range(iterations):\n",
    "        topics_title = community.louvain_communities(title_nx_graph, weight = 'weight', resolution = resolution)\n",
    "        modularity = community.modularity(title_nx_graph, topics_title, weight = 'weight', resolution = resolution)\n",
    "\n",
    "        # Check SD\n",
    "        topic_sizes = [len(c) for c in topics_title]\n",
    "        sizes_sd = np.std(topic_sizes)\n",
    "\n",
    "        topics_title_accepted.append(topics_title)\n",
    "\n",
    "        if sizes_sd < lowest_sd and min(topic_sizes) >= min_size:\n",
    "            lowest_sd_iteration = i\n",
    "            lowest_sd = sizes_sd\n",
    "      \n",
    "    # Set the chosen partitioning to be the one with highest modularity\n",
    "    topics_title = topics_title_accepted[lowest_sd_iteration]\n",
    "    print(f'Best SD: {lowest_sd}, Best iteration: {lowest_sd_iteration}, Number of topics (actual): {len(topics_title)}, Number of topics (setting): {desired_num_topics}')\n",
    "\n",
    "    topic_id_means = [sum(e)/len(e) for e in topics_title]\n",
    "    # Arrange title_topics in order of topic_id_means\n",
    "    topics_title = [list(c) for _, c in sorted(zip(topic_id_means, topics_title), key = lambda pair: pair[0])]\n",
    "    # Create an array denoting which topic each chunk belongs to\n",
    "    chunk_topics = [None] * title_similarity.shape[0]\n",
    "    for i, c in enumerate(topics_title):\n",
    "        for j in c:\n",
    "            chunk_topics[j] = i\n",
    "            \n",
    "    return {\n",
    "        'chunk_topics': chunk_topics,\n",
    "        'topics': topics_title\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GenAI summary stage 2 referencing topics and creation of articles\n",
    "def summary_stage_2(summary_data, topics, summary_num_words = 250):\n",
    "    \n",
    "    start_time = time.time()\n",
    "  \n",
    "    # Prompt to get title and summary for each chunk\n",
    "    MAP_PROMPT_TEMPLATE = \"\"\"Write a \"\"\" + str(summary_num_words) + \"\"\" word summary of the following text.  Include as much information as possible:\n",
    "  \n",
    "    {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    map_prompt = PromptTemplate(template=MAP_PROMPT_TEMPLATE, input_variables=[\"text\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = VertexAI(model_name=\"text-bison-32k\",\n",
    "        max_output_tokens=512,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        verbose=True,)\n",
    "  \n",
    "    topics_data = []\n",
    "    for c in topics:\n",
    "        topic_data_temp = [stage_1_summaries[chunk_id] for chunk_id in c]\n",
    "        topic_data_temp = '. '.join(topic_data_temp)\n",
    "        topics_data.append(topic_data_temp)\n",
    "\n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "    map_llm_chain_input = [{'text': t} for t in topics_data]\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "    stage_2_outputs_summary = [e['text'] for e in map_llm_chain_results]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'Summary creation - Stage 2: {total_time} minutes')\n",
    "\n",
    "    return {\n",
    "        'stage_2_outputs_summary': stage_2_outputs_summary\n",
    "    }\n",
    "\n",
    "def title_stage_2(summary_data, topics, summary_num_words = 250):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    # Prompt to get title and summary for each chunk\n",
    "    MAP_PROMPT_TEMPLATE = \"\"\"Write a short title of the following text.  Do not include an explanation:\n",
    "\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    map_prompt = PromptTemplate(template=MAP_PROMPT_TEMPLATE, input_variables=[\"text\"])\n",
    "\n",
    "    # Define the LLMs\n",
    "    map_llm = VertexAI(model_name=\"text-bison-32k\",\n",
    "        max_output_tokens=256,\n",
    "        temperature=0.4,\n",
    "        top_p=0.8,\n",
    "        top_k=40,\n",
    "        verbose=True,)\n",
    "        \n",
    "    map_llm_chain = LLMChain(llm = map_llm, prompt = map_prompt)\n",
    "    map_llm_chain_input = [{'text': t} for t in summary_data]\n",
    "    # Run the input through the LLM chain (works in parallel)\n",
    "    map_llm_chain_results = map_llm_chain.apply(map_llm_chain_input)\n",
    "\n",
    "    stage_2_outputs_title = [e['text'] for e in map_llm_chain_results]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'Title creation - Stage 2: {total_time} minutes')\n",
    "\n",
    "    return {\n",
    "        'stage_2_outputs_title': stage_2_outputs_title\n",
    "    }\n",
    "\n",
    "def article_stage_2(stage_1_summaries, num_drafts=1):\n",
    "    # set number of drafts to generate\n",
    "    NUM_DRAFTS = num_drafts\n",
    "\n",
    "    # Query Palm 2 to get a summary and title for each topic\n",
    "    drafts = []\n",
    "    for i in range(0, NUM_DRAFTS):\n",
    "        print(f\"Draft {i}\")\n",
    "        stage_2_outputs_summary = summary_stage_2(stage_1_summaries, topics, summary_num_words = 250)['stage_2_outputs_summary']\n",
    "        stage_2_outputs_title = title_stage_2(stage_2_outputs_summary, topics, summary_num_words = 250)['stage_2_outputs_title']\n",
    "        drafts.append([i+1, stage_2_outputs_title, stage_2_outputs_summary])\n",
    "    \n",
    "    return {\n",
    "        'drafts': drafts\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data: Get data from GCS\n",
    "\n",
    "Can loop through GCS to pick up multiple files.  This iteration is built for one file at a time but easy to update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through GCS bucket(s) for audio files to transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mg-ce-demos/audio_data/test/Mapping_uncharted_undersea_volcanoes,_and_elephant_seals_dive_deep_to_sleep.mp3\n"
     ]
    }
   ],
   "source": [
    "sample_uri = get_audio_data(GCS_BUCKET, BLOB_PATH)['data_uri']\n",
    "print(sample_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data: Transcribe audio and stringify\n",
    "\n",
    "Until we can associate specific comments with specific people (e.g. using something like Speaker ID), speaker diarization doesn't add much value.  In v2, this was removed so we could test with the new Chirp model which did appear to improve accuracy of the transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to transcribe audio, including speaker diarization and other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table mg-ce-demos.stt_genai.transcript_stt_genai_demo_chirp already exists.\n",
      "The table is not empty\n"
     ]
    }
   ],
   "source": [
    "## BQ table check\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.{DATASET}.{TRANSCRIPT_TABLE_CHIRP}\"\n",
    "\n",
    "prior_transcription=False\n",
    "try:\n",
    "    bq.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    if bq.query(f\"SELECT COUNT(*) FROM {table_id}\").result().total_rows == 0:\n",
    "        print(\"The table is empty\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        response = transcribe_gcs_v2(sample_uri)\n",
    "        \n",
    "        order = 0\n",
    "\n",
    "        transcript_df2 = pd.DataFrame(columns=['file', 'order', 'text'])\n",
    "\n",
    "        for result in response.results[sample_uri].transcript.results:\n",
    "            transcript_df2.loc[len(transcript_df2.index)] = [sample_uri, order, result.alternatives[0].transcript]\n",
    "            order += 1\n",
    "\n",
    "        transcript_df2.head()\n",
    "        transcript_string = transcript_df2['text'].str.cat(sep=' ')\n",
    "\n",
    "        table_id = f'{PROJECT_ID}.{DATASET}.{TRANSCRIPT_TABLE_CHIRP}'\n",
    "\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "        )\n",
    "\n",
    "        job = bq.load_table_from_dataframe(\n",
    "            transcript_df2, table_id, job_config=job_config\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete.\n",
    "        table = bq.get_table(table_id)  # Make an API request.\n",
    "        print(\n",
    "            \"Loaded {} rows and {} columns to {}\".format(\n",
    "                table.num_rows, len(table.schema), table_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = (end_time - start_time)/60\n",
    "        print(f'{total_time} minutes')\n",
    "    else:\n",
    "        print(\"The table is not empty\")\n",
    "        prior_transcription=True\n",
    "        query = \"\"\"\n",
    "        SELECT\n",
    "          *\n",
    "        FROM `{0}`\n",
    "        \"\"\".format(table_id)\n",
    "        transcript_df2 = bq.query(query).to_dataframe()\n",
    "        transcript_df2 = transcript_df2.sort_values(by=['order'])\n",
    "        transcript_string = transcript_df2['text'].str.cat(sep=' ')\n",
    "        transcript_string = transcript_string.replace('a.m.', 'am')\n",
    "        transcript_string = transcript_string.replace('p.m.', 'pm')\n",
    "        transcript_string = transcript_string.replace('.com', ' dot com')\n",
    "\n",
    "except NotFound:\n",
    "    print(\"Table {} is not found.\".format(table_id))\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = transcribe_gcs_v2(sample_uri)\n",
    "    \n",
    "    order = 0\n",
    "\n",
    "    transcript_df2 = pd.DataFrame(columns=['file', 'order', 'text'])\n",
    "\n",
    "    for result in response.results[sample_uri].transcript.results:\n",
    "        transcript_df2.loc[len(transcript_df2.index)] = [sample_uri, order, result.alternatives[0].transcript]\n",
    "        order += 1\n",
    "        \n",
    "    transcript_df2.head()\n",
    "    transcript_string = transcript_df2['text'].str.cat(sep=' ')\n",
    "    transcript_string = transcript_string.replace('a.m.', 'am')\n",
    "    transcript_string = transcript_string.replace('p.m.', 'pm')\n",
    "    transcript_string = transcript_string.replace('.com', ' dot com')\n",
    "    transcript_string = transcript_string.replace('.org', ' dot org')\n",
    "    \n",
    "    table_id = f'{PROJECT_ID}.{DATASET}.{TRANSCRIPT_TABLE_CHIRP}'\n",
    "\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.job.WriteDisposition.WRITE_TRUNCATE\n",
    "    )\n",
    "\n",
    "    job = bq.load_table_from_dataframe(\n",
    "        transcript_df2, table_id, job_config=job_config\n",
    "    )\n",
    "    job.result()  # Wait for the job to complete.\n",
    "    table = bq.get_table(table_id)  # Make an API request.\n",
    "    print(\n",
    "        \"Loaded {} rows and {} columns to {}\".format(\n",
    "            table.num_rows, len(table.schema), table_id\n",
    "        )\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = (end_time - start_time)/60\n",
    "    print(f'{total_time} minutes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build transcription, by speaker, grabbing words passing the confidence threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35973"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transcript_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 32K Model Summarization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgen_model_32k = TextGenerationModel_preview.from_pretrained('text-bison-32k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_string_32k = transcript_string[0:30000]\n",
    "len(transcript_string_32k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " - The number of known seamounts nearly doubled to 19,000 with the help of radar-equipped satellites that measure the height of the ocean worldwide. \n",
       " - Seamounts are important because they provide unique habitats for sea life, can affect ocean circulation, and are potential hazards for submarines and ships. \n",
       " - Elephant seals can sleep while diving underwater by flipping upside down and spinning in a circle. \n",
       " - This behavior allows them to get deep sleep while still being able to breathe. \n",
       " - Researchers used sleep monitors and motion sensors to study the sleep patterns of elephant seals in the wild. \n",
       " - Addiction is a chronic mental health condition that affects millions of people worldwide. \n",
       " - It is defined as a compulsive behavior that persists despite negative consequences. \n",
       " - The number of addiction-related deaths has increased dramatically in recent years, with over 100,000 Americans dying each year from drug overdoses. \n",
       " - The science of addiction has advanced significantly in recent years, with researchers gaining a better understanding of the neurobiology of addiction and the factors that contribute to it. \n",
       " - New treatments for addiction are being developed based on this research, with the goal of helping people overcome this devastating disorder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 32k model summarization test\n",
    "# Ask the LLM\n",
    "prompt = \"\"\"Summarize the key points of the text below.  Write the summary in bullet form with 1-2 sentences per key point.\n",
    "\n",
    "Text:\n",
    "{}\"\"\".format(transcript_string_32k)\n",
    "\n",
    "# Send prompt to LLM\n",
    "response_32k = textgen_model_32k.predict(\n",
    "   (prompt),\n",
    "    max_output_tokens=2000,\n",
    "    temperature=0.4,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    ")\n",
    "display(Markdown(str(response_32k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create sentences and chunks\n",
    "\n",
    "After transcription, we need to isolate sentences, and create chunks of content.  Unlikely we will get much meaningful context from individual sentences.  Also, we don't want to lose context through chunking so we also will overlap when we combine sentences (i.e. sentences->chunks 1-2-3-4, 2-3-4-5, 3-4-5-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get segments from txt by splitting on .\n",
    "segments =  transcript_string.split('.')\n",
    "# Put the . back in\n",
    "segments = [segment + '.' for segment in segments]\n",
    "# Further split by comma\n",
    "#segments = [segment.split(',') for segment in segments]\n",
    "# Flatten\n",
    "#segments = [item for sublist in segments for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chunk size and overlap\n",
    "sentences = create_sentences(segments, MIN_WORDS=10, MAX_WORDS=100)\n",
    "chunks = create_chunks(sentences, CHUNK_LENGTH=5, STRIDE=1)\n",
    "chunks_text = [chunk['text'] for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(chunks_text))\n",
    "#chunks_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Stage 1: Get Chunk Summaries\n",
    "\n",
    "Now we create our first summaries and titles based on the chunks of sentences we created from the transcriptions.  This is done using langchain and chaining prompts to create the summary and subsequently the titles.  These can be accomplished in a single step but I found the outputs not as good as doing them sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary creation - Stage 1: 5.053415668010712 minutes\n"
     ]
    }
   ],
   "source": [
    "# Run Stage 1 Summarizing\n",
    "stage_1_summaries = summary_stage_1(chunks_text)['stage_1_outputs_summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minor data cleanup\n",
    "stage_1_summaries = [\" \" if x == '' else x for x in stage_1_summaries]\n",
    "stage_1_summaries = [string.replace('\\n', ' ') for string in stage_1_summaries]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Generate embeddings for titles and summaries, and cluster into distinct topics\n",
    "\n",
    "Using the stage 1 summaries, now we group them into common topics using cosine similarity and Louvain community detection.  This is necessary as the content contains many different topics, which when combining will make it hard for LLMs to produce concise content.  This step in the process produces the final context for creating final summaries / articles for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best SD: 0.9270248108869579, Best iteration: 1, Number of topics (actual): 8, Number of topics (setting): 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set num_topics to be 1/4 of the number of chunks, or 12, which ever is smaller\n",
    "num_1_chunks = len(stage_1_summaries)\n",
    "NUM_TOPICS = min(int(num_1_chunks / 4), 12)\n",
    "topics_out = get_topics(stage_1_summaries, num_1_chunks, num_topics = NUM_TOPICS, bonus_constant = 0.3)\n",
    "chunk_topics = topics_out['chunk_topics']\n",
    "topics = topics_out['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4, 5, 6],\n",
       " [8, 9, 7],\n",
       " [10, 11, 12, 13],\n",
       " [16, 17, 14, 15],\n",
       " [18, 19, 20, 21],\n",
       " [32, 0, 31],\n",
       " [24, 25, 22, 23],\n",
       " [26, 27, 28, 29, 30]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a heatmap of this array\n",
    "plt.figure(figsize = (10, 4))\n",
    "plt.imshow(np.array(chunk_topics).reshape(1, -1), cmap = 'tab20')\n",
    "# Draw vertical black lines for every 1 of the x-axis \n",
    "for i in range(1, len(chunk_topics)):\n",
    "    plt.axvline(x = i - 0.5, color = 'black', linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stage 2: Final Summaries\n",
    "\n",
    "Using the topics identified prior to this step, we use the same prompt chaining approach to create a summary then title for each topic.  At this point, we have a **first draft** for each topic where the LLM was able to produce an informative article and title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Draft 0\n",
      "Summary creation - Stage 2: 1.539211599032084 minutes\n",
      "Title creation - Stage 2: 0.9943151036898296 minutes\n"
     ]
    }
   ],
   "source": [
    "drafts = article_stage_2(stage_1_summaries, num_drafts=1)['drafts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DRAFT direct 32K summarization"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " - The number of known seamounts nearly doubled to 19,000 with the help of radar-equipped satellites that measure the height of the ocean worldwide. \n",
       " - Seamounts are important because they provide unique habitats for sea life, can affect ocean circulation, and are potential hazards for submarines and ships. \n",
       " - Elephant seals can sleep while diving underwater by flipping upside down and spinning in a circle. \n",
       " - This behavior allows them to get deep sleep while still being able to breathe. \n",
       " - Researchers used sleep monitors and motion sensors to study the sleep patterns of elephant seals in the wild. \n",
       " - Addiction is a chronic mental health condition that affects millions of people worldwide. \n",
       " - It is defined as a compulsive behavior that persists despite negative consequences. \n",
       " - The number of addiction-related deaths has increased dramatically in recent years, with over 100,000 Americans dying each year from drug overdoses. \n",
       " - The science of addiction has advanced significantly in recent years, with researchers gaining a better understanding of the neurobiology of addiction and the factors that contribute to it. \n",
       " - New treatments for addiction are being developed based on this research, with the goal of helping people overcome this devastating disorder."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown('# DRAFT direct 32K summarization'))\n",
    "display(Markdown(str(response_32k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# DRAFT 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "###  Sarah Cresty Show: New seamounts discovered; elephant seal sleep; addiction research"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Sarah Cresty hosts a show about the discovery of 19,000 new seamounts, bringing the total number known to 21,500. The new seamounts were found using improved satellite data, which allowed scientists to see smaller seamounts than before. Seamounts are underwater volcanoes that are at least 1 kilometer high. They are important because they provide habitats for a variety of sea life and act as stepping stones for marine life to migrate across the ocean. The new global map of seamounts will help scientists better understand their role in the ocean and protect these important ecosystems.  In the first segment, Sarah Cresty interviews Paulsen about a study that nearly doubled the number of known seamounts. In the second segment, Sarah Cresty interviews Jessica Kendle Bar about her work exploring the sleep of elephant seals. In a sponsored segment, Jackie Oburst talks with addiction researchers Eric Nestler and Paul Kenny about what researchers have discovered about addiction over the last 5 years."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Sleep Patterns of Marine Mammals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Elephant seals are marine mammals that spend most of their lives in the water. This makes it difficult for them to sleep deeply, as they need to come up for air regularly. To overcome this challenge, elephant seals have evolved a unique sleep pattern. They sleep in short bursts, taking turns sleeping while the others keep watch for predators. This allows them to get the rest they need while still being able to respond to danger.\n",
       "     Other marine mammals, such as whales, dolphins, and seals, can also sleep in just half of their brain at a time. This is called unihemispheric sleep, and it allows them to keep one eye open and monitor predators while they are getting the benefits of sleep for half of their brain.\n",
       "     The first night effect is a documented phenomenon where people are slightly more aware when they sleep in a new place. This is likely due to the fact that one hemisphere of the brain is more engaged in slow-wave sleep than the other."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Elephant Seal Sleep Patterns in the Wild"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " A study was conducted on elephant seals to observe their diving patterns and behavior in their natural environment. Researchers attached sleep monitors to the seals to collect data on their behavior, including depth and duration of dives. The study revealed that the seals spent a significant amount of time at extreme depths and held their breath for extended periods. \n",
       "\n",
       "The author used data-driven animations to visualize the seals' behavior underwater, showing their movements, pitch, roll, and frequency of swimming. The animations also captured the seals' unique sleep spirals. \n",
       "\n",
       "To measure the sleep behavior of seals, the author employed sensors similar to those used in human sleep studies. The author even tested the methods on himself, floating in shallow water and attempting to sleep. \n",
       "\n",
       "Due to the size and potential danger of elephant seals, researchers employed various methods to attach sensors to them, including sedation and the use of headcaps. The study revealed that elephant seals spend most of their time in a state of slow-wave sleep, and their sleep patterns are influenced by their environment, with seals in areas with more predators spending less time in deep sleep. \n",
       "\n",
       "The study also found that elephant seals can control their buoyancy while sleeping, allowing them to maintain their desired depth in the water without constant swimming. The research provides valuable insights into the sleep patterns of elephant seals and their adaptation to their environment. \n",
       "\n",
       "Additionally, the author's use of 3D motion sensors revealed that seals enter REM sleep while upside down and spinning in a circle, suggesting their ability to sleep while still moving, which is uncommon among ocean mammals."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Sleep Patterns and Heart Rate Variability in Elephant Seals"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Elephant seals exhibit significant variation in their sleep patterns depending on their diving behavior.  When looking at the population as a whole,  \"sleep dives\" can be observed,  where the seals  sleep for short periods during their dives.  On average, elephant seals sleep for about two hours a day during long foraging trips, slightly less during shorter trips, and slightly more during extremely long trips.  However, there is a group of elephant seals that employ a different foraging strategy, traveling up the coast and diving much deeper. These seals perform only  three to ten  foraging dives a day and spend the rest of the time sleeping, sometimes up to 10 hours per day.  The heart rate of elephant seals also varies greatly,  ranging from as high as 200 beats per minute to as low as five beats per minute.  This flexibility in heart rate is likely related to the seals' diving behavior and their ability to lower their heart rate to conserve oxygen during dives.  The heart rate of seals can vary greatly depending on their age and how well they have developed their mammalian dive response, which allows them to lower their heart rate when diving.  Heart rates as high as 200 beats per minute and as low as five beats per minute have been recorded.  The lowest heart rates were recorded after the seals encountered a killer whale and immediately dove back down.  Studying the sleep patterns of seals in the wild is important for understanding how, where, and when they rest, and this information can be used to better manage and protect their critical resting habitats."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Addiction: A Complex and Multifaceted Disease"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Summary:\n",
       "  \n",
       "Jessica Kendall Bar is a Schmitt AI and Science post-doctoral Fellow at the Scripps Institution of Oceanography at UC San Diego. She and her colleagues recently published a paper in Science magazine discussing the latest research on addiction. In the paper, Bar and her colleagues argue that addiction is a chronic disease that affects the brain and behavior. They also discuss the importance of understanding the underlying mechanisms of addiction in order to develop more effective treatments.  \n",
       "\n",
       "Addiction is a chronic mental health condition that kills hundreds of thousands of Americans every year and impacts millions of lives. It can destroy marriages, friendships, and careers, and threaten a person's basic health and safety. Addiction is a medical problem that needs medical solutions.  \n",
       "\n",
       "Dr. Nora Volkow, Director of the National Institute on Drug Abuse, and Dr. Charles O'Brien, Professor of Psychiatry at the University of Pennsylvania, are two renowned experts on addiction. They discuss the latest research on addiction and how it can be treated.  \n",
       "\n",
       "Addiction is a complex and multifaceted disease that affects millions of people around the world. It is characterized by compulsive drug seeking and use, despite negative consequences. The science of addiction is a rapidly growing field, and new insights into the underlying mechanisms of addiction are emerging all the time.  \n",
       "\n",
       "Dr. Eric Nessler and Dr. Paul Kenny are two renowned experts on addiction who will be discussing the latest research on addiction at an upcoming event. Dr. Nessler's laboratory studies the molecular mechanisms of drug addiction and depression in animal models. Dr. Kenny's research focuses on the development of new treatments for addiction.  \n",
       "\n",
       "Addiction is a psychiatric syndrome that can only be diagnosed by talking to a person or their family members and finding out what types of abnormal behaviors they exhibit. Researchers define addiction in different ways, which is one of the major gaps in our knowledge.  \n",
       "\n",
       "Dr. Eric Nestler and Dr. Paul Kenny are two researchers who study addiction. Dr. Nestler's research focuses on the molecular underpinnings of neurobehavioral disorders such as schizophrenia and drug addiction. Dr. Kenny's research involves the study of behavioral paradigms, physiological analyses, and the molecular underpinnings of neurobehavioral disorders such as schizophrenia and drug addiction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Science Podcast: The Science of Sound"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " This week's Science Podcast explores the science of sound. The episode covers a range of topics related to sound, including how sound waves work, how we hear sound, and how sound can be used to create music and other forms of art. The episode is hosted by Sarah Cresby and Kevin McClain, and features music by Jeffrey Cook.\n",
       "This episode of the Science Podcast is brought to you by the Icahn School of Medicine at Mount Sinai. Researchers at the Icahn School of Medicine are working to advance our understanding of the brain and improve care for disorders such as depression, dementia, and drug addiction. \n",
       "You can learn more about their work in a special supplement to Science magazine, prepared by Icahn Mount Sinai in partnership with Science. \n",
       "Visit www.science.org and search for \"Frontiers of Medical Research-Brain Science.\"\n",
       "The Science Podcast is a production of Science magazine, and is available on their website as well as on podcasting apps. \n",
       "Listener feedback is welcome at  SciencePodcast@aaas.org."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  The American Drug Epidemic"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Since the pandemic, the number of American fatalities from drug overdoses has skyrocketed, reaching an estimated 100,000 per year. This surpasses the number of American soldiers killed in the Vietnam War. The primary cause of this rise is the widespread use of opioids in the United States, but other drugs like cocaine and methamphetamine also contribute. \n",
       "     We have made great strides in understanding the science of addiction in recent years. Drugs that people abuse can activate the brain's reward pathways, causing changes that can result in addiction. Addiction is a chronic disease that necessitates medical attention. Diagnosis of addiction, a psychiatric condition, requires a conversation to identify abnormal behaviors as there is no definitive blood test, genetic test, or brain scan. \n",
       "     Addiction is a compulsive behavior that persists despite negative consequences and is aimed at obtaining drugs.  The consequences of addiction on society are severe. The opioid epidemic is a major contributor to premature death in the United States, claiming nearly 71,000 lives in 2019.  Addiction can result in higher crime rates, unemployment, lower educational attainment, increased healthcare expenses, social isolation, and mental health issues."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "###  Rewarding Pathways and Addiction"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       " Reward pathways are ancient parts of the brain that regulate responses to rewarding stimuli. Drugs of abuse hijack these pathways, creating an intensely rewarding experience without any actual benefit. This can lead to addiction, as the brain's reward system becomes corrupted and natural rewards are no longer satisfying. Addicted individuals require the powerful effects of drugs to feel normal. Drug abuse causes chemical changes in the brain that drive addiction, and genetic variations can influence the likelihood of drug use, addiction, and difficulty quitting. Medications like methadone and suboxone can be effective in treating opioid addiction, but new therapeutics are needed. Behavioral therapies are the mainstay of addiction treatment but have high relapse rates. Combining behavioral therapy with medication is currently the most effective treatment for substance use disorders.  Despite the challenges, most people eventually overcome addiction."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(drafts)):\n",
    "    display(Markdown('# DRAFT ' + str(drafts[i][0])))\n",
    "    for j in range(len(drafts[i][1])):\n",
    "        if drafts[i][1][j] != \"\":\n",
    "            if drafts[i][2][j] != \"\":\n",
    "                display(Markdown('### ' + drafts[i][1][j]))\n",
    "                display(Markdown(drafts[i][2][j]))\n",
    "                #print(drafts[i][2][j])\n",
    "                print('------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## UI (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file):\n",
    "    file_path = file.name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    bucket = gcs.bucket(GCS_BUCKET)\n",
    "    blob = bucket.blob(f'audio_data/cumulus/{file_name}')\n",
    "    blob.upload_from_filename(f'{file_path}')\n",
    "    #print(\"Uploaded\")\n",
    "    \n",
    "    gcs_path = f'gs://{GCS_BUCKET}/{blob.name}'\n",
    "    \n",
    "    return gcs_path\n",
    "    \n",
    "#with gr.Blocks() as demo:\n",
    "#    file_output = gr.File()\n",
    "#    upload_button = gr.UploadButton(\"Click to Upload a File\", file_types=[\"audio\"])\n",
    "#    upload_button.upload(upload_file, upload_button, file_output)\n",
    "\n",
    "#demo.launch(share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (754168372.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[32], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# create a function that pulls all the steps together\n",
    "def article_builder(gcs_uri):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_audio_data('mg-ce-demos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "    text = gr.Textbox()\n",
    "    label = gr.Label()\n",
    "\n",
    "    b1 = gr.Button(\"Upload\")\n",
    "    b2 = gr.Button(\"Get GCS URI\")\n",
    "    b3 = gr.Button(\"Create Articles\")\n",
    "\n",
    "    b1.click(upload_file, inputs=audio_file, outputs=text)\n",
    "    b2.click(get_audio_data, inputs=text, outputs=label)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\n",
    "    \"\"\"\n",
    "    ## Audio to Articles\n",
    "    \"\"\")\n",
    "    #with gr.Row():\n",
    "    #    file_output = gr.File()\n",
    "    #    upload_button = gr.UploadButton(\"Click to Transcribe and Draft Articles\", file_types=[\"audio\"])\n",
    "    #    upload_button.upload(upload_file, upload_button, file_output)  \n",
    "        \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            input_text = gr.Textbox(label=\"Task\", placeholder=\"Cloud Bucket Name\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        find_gcs_uri = gr.Button(\"Select Audio File\")\n",
    "        \n",
    "    with gr.Row():\n",
    "        label1 = gr.Textbox(label=\"GCS URI\")\n",
    "        \n",
    "    #with gr.Row():\n",
    "    #    generate = gr.Button(\"Generate Response\")\n",
    "\n",
    "    #with gr.Row():\n",
    "    #    label2 = gr.Textbox(label=\"Prompt\")\n",
    "    #with gr.Row():\n",
    "    #    label3 = gr.Textbox(label=\"Response generated by LLM\")\n",
    "\n",
    "    generate.click(get_audio_data, input_text, label1)\n",
    "    \n",
    "demo.launch(share=False, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "d228f8d1b134e326a52396b2016d42a4e7c84199cf5eb27412c1836171e03131"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
