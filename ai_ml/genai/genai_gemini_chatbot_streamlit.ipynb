{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80c2a04-14dc-4f7d-aab2-60d0015ad896",
   "metadata": {},
   "source": [
    "# Gemini Chatbot Starter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a99e24-9c99-4742-ac69-f1d4c8d407a1",
   "metadata": {},
   "source": [
    "## Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af2b8dbc-258d-46d5-94b7-9de6e1a6a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting geminibot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile geminibot.py\n",
    "\n",
    "import streamlit as st\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.generative_models\n",
    "from vertexai.generative_models import GenerativeModel, Part, Content\n",
    "import tempfile\n",
    "import os\n",
    "import mimetypes\n",
    "from typing import Iterator\n",
    "import time\n",
    "\n",
    "PROJECT_ID = 'mg-ce-demos' # change to your GCP project ID\n",
    "REGION = 'us-central1' # change to the appropriate region\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "def process_uploaded_file(uploaded_file) -> Part:\n",
    "    \"\"\"Process uploaded file and convert to Gemini Part object.\"\"\"\n",
    "    if uploaded_file is None:\n",
    "        return None\n",
    "        \n",
    "    # Create a temporary file to store the uploaded content\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(uploaded_file.getvalue())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Determine mime type\n",
    "    mime_type, _ = mimetypes.guess_type(uploaded_file.name)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"\n",
    "\n",
    "    # Create Part object from file\n",
    "    with open(tmp_file_path, \"rb\") as f:\n",
    "        file_content = f.read()\n",
    "        \n",
    "    # Clean up temporary file\n",
    "    os.unlink(tmp_file_path)\n",
    "    \n",
    "    return Part.from_data(data=file_content, mime_type=mime_type)\n",
    "\n",
    "def stream_chat(\n",
    "    chat_history: list, \n",
    "    query: str, \n",
    "    uploaded_part: Part = None, \n",
    "    model=vertexai.generative_models.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    ") -> Iterator[str]:\n",
    "    \"\"\"Stream chat responses from Gemini.\"\"\"\n",
    "    # Initialize chat\n",
    "    formatted_history = []\n",
    "    for msg in chat_history:\n",
    "        formatted_history.extend([\n",
    "            Content(role=\"user\", parts=[Part.from_text(str(msg[\"user\"]))]),\n",
    "            Content(role=\"model\", parts=[Part.from_text(str(msg[\"model\"]))])\n",
    "        ])\n",
    "    \n",
    "    chat = model.start_chat(history=formatted_history)\n",
    "    \n",
    "    # Prepare message parts\n",
    "    message_parts = [query]\n",
    "    if uploaded_part:\n",
    "        message_parts.insert(0, uploaded_part)\n",
    "    \n",
    "    # Get streaming response\n",
    "    response = chat.send_message(message_parts, stream=True)\n",
    "    \n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, \"text\"):\n",
    "            yield chunk.text\n",
    "\n",
    "def main():\n",
    "    st.title(\"Chat with Gemini\")\n",
    "\n",
    "    # Add clear history button in the sidebar\n",
    "    with st.sidebar:\n",
    "        model_name = st.selectbox(\n",
    "            \"Select Gemini Model\",\n",
    "            [\"gemini-2.0-flash-exp\", \"gemini-1.5-pro-002\", \"gemini-1.5-flash-002\"],\n",
    "            index=0\n",
    "        )\n",
    "        system_prompt = st.text_area(\"System Prompt (optional)\")\n",
    "        if st.button(\"Clear Chat History\"):\n",
    "            st.session_state.chat_history = []\n",
    "            st.rerun()\n",
    "\n",
    "    # Check if system prompt is empty\n",
    "    if len(system_prompt) > 0:\n",
    "        MODEL = vertexai.generative_models.GenerativeModel(model_name, system_instruction=system_prompt)\n",
    "    else:\n",
    "        MODEL = vertexai.generative_models.GenerativeModel(model_name)\n",
    "    \n",
    "    # Initialize session state for chat history\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "    \n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Upload a file (optional)\", type=[\"txt\", \"pdf\", \"png\", \"jpg\", \"jpeg\"])\n",
    "    \n",
    "    # Convert uploaded file to Part if present\n",
    "    uploaded_part = process_uploaded_file(uploaded_file) if uploaded_file else None\n",
    "    \n",
    "    # Chat input\n",
    "    user_input = st.chat_input(\"Type your message here...\")\n",
    "    \n",
    "    # Display chat history\n",
    "    for message in st.session_state.chat_history:\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(message[\"user\"])\n",
    "        with st.chat_message(\"model\"):\n",
    "            st.write(message[\"model\"])\n",
    "    \n",
    "    # Handle new user input\n",
    "    if user_input:\n",
    "        # Show user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(user_input)\n",
    "            \n",
    "        # Show assistant response with streaming\n",
    "        with st.chat_message(\"model\"):\n",
    "            message_placeholder = st.empty()\n",
    "            full_response = \"\"\n",
    "            \n",
    "            # Stream the response\n",
    "            for chunk in stream_chat(st.session_state.chat_history, user_input, uploaded_part, model=MODEL):\n",
    "                full_response += chunk\n",
    "                message_placeholder.markdown(full_response + \"â–Œ\")\n",
    "                time.sleep(0.05)\n",
    "            \n",
    "            # Update final response\n",
    "            message_placeholder.markdown(full_response)\n",
    "        \n",
    "        # Update chat history\n",
    "        st.session_state.chat_history.append({\n",
    "            \"user\": user_input,\n",
    "            \"model\": full_response\n",
    "        })\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff11b42-fc1e-4739-845b-6f3211f0d3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245e054-a7f2-4012-8a69-9abd5355edf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
