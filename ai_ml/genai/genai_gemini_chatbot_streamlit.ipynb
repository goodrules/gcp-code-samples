{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c80c2a04-14dc-4f7d-aab2-60d0015ad896",
   "metadata": {},
   "source": [
    "# GenAI Prompt Engineering Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67be22b-f563-4b0b-ab3d-8866723d2102",
   "metadata": {},
   "source": [
    "---\n",
    "## Installs\n",
    "\n",
    "The list `packages` contains tuples of package import names and install names.  If the import name is not found then the install name is used to install quitely for the current user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b15cbe64-1a11-41a6-b09a-93d7e8dcc99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tuples of (import name, install name, min_version)\n",
    "packages = [\n",
    "    ('google.cloud.aiplatform', 'google-cloud-aiplatform'),\n",
    "    ('google.cloud.storage', 'google-cloud-storage'),\n",
    "    ('google.cloud.bigquery', 'google-cloud-bigquery'),\n",
    "    ('streamlit', 'streamlit')\n",
    "]\n",
    "\n",
    "import importlib\n",
    "install = False\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package[0]):\n",
    "        print(f'installing package {package[1]}')\n",
    "        install = True\n",
    "        !pip install {package[1]} -U -q --user\n",
    "    elif len(package) == 3:\n",
    "        if importlib.metadata.version(package[0]) < package[2]:\n",
    "            print(f'updating package {package[1]}')\n",
    "            install = True\n",
    "            !pip install {package[1]} -U -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeeb94-0e3e-41b1-9f1d-2dd2fad2a515",
   "metadata": {},
   "source": [
    "### Restart Kernel (If Installs Occured)\n",
    "\n",
    "After a kernel restart the code submission can start with the next cell after this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05eb94fd-0562-4e3c-a63b-4952b5402519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if install:\n",
    "    import IPython\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5107e967-b47b-43d4-8098-b0e5d2262d49",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93da7af-6765-4730-b6d0-d7e7e188f79c",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e11fd07-a9b3-428f-9ee9-16ee36179798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mg-ce-demos'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f7dba0-40ea-4a87-88cd-d7762ab96ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "\n",
    "# Set the BUCKET name for saving work:\n",
    "BUCKET = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eda4f2-665e-4d72-84f7-dc1db3821348",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f694039-e4ac-4376-bb8c-234ac30b4d72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "import vertexai.vision_models # Imagen Models\n",
    "import vertexai.preview.vision_models\n",
    "import vertexai.language_models # PaLM and Codey Models\n",
    "import vertexai.generative_models # for Gemini Models\n",
    "\n",
    "import json\n",
    "import io\n",
    "import base64\n",
    "import asyncio\n",
    "import requests\n",
    "import IPython\n",
    "import datetime, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55e1c35c-3900-462c-86a4-e590d5eaedf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.71.1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fd992-fc1e-41aa-ac50-8ce7334057fd",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf195c7-4dff-4370-9dca-d49b90792578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "bq = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "bucket = gcs.lookup_bucket(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd89fd8d-1a84-4077-b84a-30ce99be6ed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gemini Models\n",
    "gemini15_multimodal = vertexai.generative_models.GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "gemini_flash = vertexai.generative_models.GenerativeModel(\"gemini-1.5-flash-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a99e24-9c99-4742-ac69-f1d4c8d407a1",
   "metadata": {},
   "source": [
    "## Streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af2b8dbc-258d-46d5-94b7-9de6e1a6a81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting geminibot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile geminibot.py\n",
    "\n",
    "import streamlit as st\n",
    "from google.cloud import aiplatform\n",
    "import vertexai.generative_models\n",
    "from vertexai.generative_models import GenerativeModel, Part, Content\n",
    "import tempfile\n",
    "import os\n",
    "import mimetypes\n",
    "from typing import Iterator\n",
    "\n",
    "PROJECT_ID = 'mg-ce-demos'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "# Initialize Gemini 1.5 Pro model\n",
    "model = vertexai.generative_models.GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "def process_uploaded_file(uploaded_file) -> Part:\n",
    "    \"\"\"Process uploaded file and convert to Gemini Part object.\"\"\"\n",
    "    if uploaded_file is None:\n",
    "        return None\n",
    "        \n",
    "    # Create a temporary file to store the uploaded content\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "        tmp_file.write(uploaded_file.getvalue())\n",
    "        tmp_file_path = tmp_file.name\n",
    "\n",
    "    # Determine mime type\n",
    "    mime_type, _ = mimetypes.guess_type(uploaded_file.name)\n",
    "    if mime_type is None:\n",
    "        mime_type = \"application/octet-stream\"\n",
    "\n",
    "    # Create Part object from file\n",
    "    with open(tmp_file_path, \"rb\") as f:\n",
    "        file_content = f.read()\n",
    "        \n",
    "    # Clean up temporary file\n",
    "    os.unlink(tmp_file_path)\n",
    "    \n",
    "    return Part.from_data(data=file_content, mime_type=mime_type)\n",
    "\n",
    "def stream_chat(chat_history: list, query: str, uploaded_part: Part = None) -> Iterator[str]:\n",
    "    \"\"\"Stream chat responses from Gemini.\"\"\"\n",
    "    # Initialize chat\n",
    "    formatted_history = []\n",
    "    for msg in chat_history:\n",
    "        formatted_history.extend([\n",
    "            Content(role=\"user\", parts=[Part.from_text(str(msg[\"user\"]))]),\n",
    "            Content(role=\"model\", parts=[Part.from_text(str(msg[\"model\"]))])\n",
    "        ])\n",
    "    \n",
    "    chat = model.start_chat(history=formatted_history)\n",
    "    \n",
    "    # Prepare message parts\n",
    "    message_parts = [query]\n",
    "    if uploaded_part:\n",
    "        message_parts.insert(0, uploaded_part)\n",
    "    \n",
    "    # Get streaming response\n",
    "    response = chat.send_message(message_parts, stream=True)\n",
    "    \n",
    "    for chunk in response:\n",
    "        if hasattr(chunk, \"text\"):\n",
    "            yield chunk.text\n",
    "\n",
    "def main():\n",
    "    st.title(\"Gemini 1.5 Pro Chat\")\n",
    "\n",
    "    # Add clear history button in the sidebar\n",
    "    if st.sidebar.button(\"Clear Chat History\"):\n",
    "        st.session_state.chat_history = []\n",
    "        st.rerun()\n",
    "    \n",
    "    # Initialize session state for chat history\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = []\n",
    "    \n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Upload a file (optional)\", type=[\"txt\", \"pdf\", \"png\", \"jpg\", \"jpeg\"])\n",
    "    \n",
    "    # Convert uploaded file to Part if present\n",
    "    uploaded_part = process_uploaded_file(uploaded_file) if uploaded_file else None\n",
    "    \n",
    "    # Chat input\n",
    "    user_input = st.chat_input(\"Type your message here...\")\n",
    "    \n",
    "    # Display chat history\n",
    "    for message in st.session_state.chat_history:\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(message[\"user\"])\n",
    "        with st.chat_message(\"model\"):\n",
    "            st.write(message[\"model\"])\n",
    "    \n",
    "    # Handle new user input\n",
    "    if user_input:\n",
    "        # Show user message\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.write(user_input)\n",
    "            \n",
    "        # Show assistant response with streaming\n",
    "        with st.chat_message(\"model\"):\n",
    "            message_placeholder = st.empty()\n",
    "            full_response = \"\"\n",
    "            \n",
    "            # Stream the response\n",
    "            for chunk in stream_chat(st.session_state.chat_history, user_input, uploaded_part):\n",
    "                full_response += chunk\n",
    "                message_placeholder.markdown(full_response + \"▌\")\n",
    "            \n",
    "            # Update final response\n",
    "            message_placeholder.markdown(full_response)\n",
    "        \n",
    "        # Update chat history\n",
    "        st.session_state.chat_history.append({\n",
    "            \"user\": user_input,\n",
    "            \"model\": full_response\n",
    "        })\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff11b42-fc1e-4739-845b-6f3211f0d3f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e245e054-a7f2-4012-8a69-9abd5355edf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
