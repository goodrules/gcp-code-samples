{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badede1e-c779-4653-bb1c-9837b0b15a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410767ac-d254-4280-94d3-cbf979efa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import register_matplotlib_converters\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87984414-37b7-42db-b80b-7a0109cec7bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Set project and model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974a309a-487b-4d44-9b32-aa512673eecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your project and region. Then run the  cell to make sure the\n",
    "# Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "PROJECT = 'mg-ce-demos' # REPLACE WITH YOUR PROJECT NAME \n",
    "REGION = 'us-central1' # REPLACE WITH YOUR REGION e.g. us-central1\n",
    "BUCKET = 'mg-ce-demos-bucket' # REPLACE WITH A UNIQUE BUCKET NAME e.g. your PROJECT NAME\n",
    "BUCKET_URI = 'gs://' + BUCKET\n",
    "\n",
    "#Don't change the following command - this is to check if you have changed the project name above.\n",
    "assert PROJECT != 'your-project-name', 'Don''t forget to change the project variables!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56000fd1-8ddd-4438-8300-aacbccfbea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'total_rides' # The variable you are predicting\n",
    "target_description = 'Total Rides' # A description of the target variable\n",
    "features = {'day_type': 'Day Type'} # Weekday = W, Saturday = A, Sunday/Holiday = U\n",
    "ts_col = 'service_date' # The name of the column with the date field\n",
    "\n",
    "raw_data_file = 'https://data.cityofchicago.org/api/views/6iiy-9s97/rows.csv?accessType=DOWNLOAD'\n",
    "processed_file = 'cta_ridership.csv' # Which file to save the results to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754fbe6d-2cc3-4efb-ad0b-7cc8f11ad85f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d18cd-7b4e-45bd-abf0-f899bd4d66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CSV file\n",
    "\n",
    "df = pd.read_csv(raw_data_file, index_col=[ts_col], parse_dates=[ts_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a42cc4-a5a2-45fe-b5d4-26eb53a6449b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model data prior to 2020 \n",
    "\n",
    "df = df[df.index < '2020-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442a23f-36aa-402d-a66a-b2d723a731e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9799e90-5b33-44da-8c8b-0d76ab8a4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date\n",
    "\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eaa940-3b74-42be-9ad2-6b8e4213dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 rows\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5afd70-5471-4e92-b0a4-2d7e67775821",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88ff1b-a974-4a11-80bf-aa072f433b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize plotting\n",
    "\n",
    "register_matplotlib_converters() # Addresses a warning\n",
    "sns.set(rc={'figure.figsize':(16,4)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86c9bf-37ac-4911-b315-7d2655e573e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore total rides over time\n",
    "\n",
    "sns.lineplot(data=df, x=df.index, y=df[target]).set_title('Total Rides')\n",
    "fig = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17be03-149e-42df-b8ea-93cf89b3ccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore rides by day type: Weekday (W), Saturday (A), Sunday/Holiday (U)\n",
    "\n",
    "sns.lineplot(data=df, x=df.index, y=df[target], hue=df['day_type']).set_title('Total Rides by Day Type')\n",
    "fig = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947f09c-011f-46d7-a361-b65aa178562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore rides by transportation type\n",
    "\n",
    "sns.lineplot(data=df[['bus','rail_boardings']]).set_title('Total Rides by Transportation Type')\n",
    "fig = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd944b-30f8-42bb-8ed4-35349af287d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "\n",
    "df[target].describe().apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d4434-933c-4fb9-8bf3-256db04c4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of values for each day of the week in a boxplot:\n",
    "# Min, 25th percentile, median, 75th percentile, max \n",
    "\n",
    "daysofweek = df.index.to_series().dt.dayofweek\n",
    "\n",
    "fig = sns.boxplot(x=daysofweek, y=df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9f7954-b785-41e9-9819-52f964e7a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distribution of values for each month in a boxplot:\n",
    "\n",
    "months = df.index.to_series().dt.month\n",
    "\n",
    "fig = sns.boxplot(x=months, y=df[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1568173-b7c7-4ab0-9c97-dac231e33e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the data into trend and seasonal components\n",
    "\n",
    "result = seasonal_decompose(df[target], period=365)\n",
    "fig = result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff84fe8-ba22-42f6-bca6-ef848101b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-correlation\n",
    "\n",
    "plot_acf(df[target])\n",
    "\n",
    "fig = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21ed35a-3e1b-4244-9bd0-869cb2290079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[[target]].to_csv(processed_file, index=True, index_label=ts_col)  # creates CSV if needed\n",
    "df = df[[target]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18415e26-028d-4034-9377-5130184a4f5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Custom Forecasting ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c4ebb5-7583-40ef-83ce-f90942c38828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from google.cloud import storage\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, Flatten, LSTM, MaxPooling1D\n",
    "\n",
    "register_matplotlib_converters() # Address warning\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dd36f5-9c19-4756-8776-6da0ceeba102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "target_col = 'total_rides' # The variable you are predicting\n",
    "ts_col = 'service_date' # The name of the column with the date field\n",
    "\n",
    "# Model parameters\n",
    "freq = 'D' # Daily frequency\n",
    "n_input_steps = 30 # Lookback window\n",
    "n_output_steps = 7 # How many steps to predict forward\n",
    "n_seasons = 7 # Monthly periodicity\n",
    "\n",
    "train_split = 0.8 # % Split between train/test data\n",
    "epochs = 1000 # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = 5 # Terminate training after the validation loss does not decrease after this many epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629061a-652c-4f77-84c4-daac7a16daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "try:\n",
    "    bucket = storage_client.get_bucket(BUCKET)\n",
    "    print('Bucket exists, let''s not recreate it.')\n",
    "except:\n",
    "    bucket = storage_client.create_bucket(BUCKET)\n",
    "    print('Created bucket: ' + BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d59b4-0cc8-410e-a612-16906f5a1dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.freq = freq\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0738b84c-727c-44fc-83f3-2082c57346ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some characteristics of the data that will be used later\n",
    "n_features = len(df.columns)\n",
    "\n",
    "# Index of target column. Used later when creating dataframes.\n",
    "target_col_num = df.columns.get_loc(target_col)\n",
    "\n",
    "# Split data\n",
    "size = int(len(df) * train_split)\n",
    "df_train, df_test = df[0:size].copy(deep=True), df[size:len(df)].copy(deep=True)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afead39f-ac81-4ed0-b97a-c462ef17e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe797e3-aa5a-44b0-8d35-8fcf078f261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers\n",
    "_=sns.lineplot(data=df_train[target_col]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034b89e-72c2-431b-9854-a1605fba6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 values\n",
    "df[target_col].sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31b72b8-ad05-4bd0-af7c-0988b0808a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the threshold below to remove the outliers\n",
    "\n",
    "threshold = 2000000 # Set this just below the level you are seeing peaks. It will flag any values above it.\n",
    "assert threshold != -1, 'Set the threshold to the minimum that will eliminate outlier(s)'\n",
    "\n",
    "# Set any values above the threshold to NaN (not a number)\n",
    "df_train.loc[df_train[target_col] > threshold, target_col] = np.nan\n",
    "\n",
    "# Interpolate the missing values (e.g. [3, NaN, 5] becomes [3, 4, 5])\n",
    "df_train = df_train.interpolate()\n",
    "\n",
    "# Review the updated chart to see if outliers still exist\n",
    "_=sns.lineplot(data=df_train[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7f011a-4786-4452-a49c-df06d193827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For neural networks to converge quicker, it is helpful to scale the values.\n",
    "# For example, each feature might be transformed to have a mean of 0 and std. dev. of 1.\n",
    "#\n",
    "# You are working with a mix of features, input timesteps, output horizon, etc.\n",
    "# which don't work out-of-the-box with common scaling utilities.\n",
    "# So, here are a couple wrappers to handle scaling and inverting the scaling.\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "def scale(df, \n",
    "          fit=True, \n",
    "          target_col=target_col,\n",
    "          feature_scaler=feature_scaler,\n",
    "          target_scaler=target_scaler):\n",
    "    \"\"\"\n",
    "    Scale the input features, using a separate scaler for the target.\n",
    "    \n",
    "    Parameters: \n",
    "    df (pd.DataFrame): Input dataframe\n",
    "    fit (bool): Whether to fit the scaler to the data (only apply to training data)\n",
    "    target_col (pd.Series): The column that is being predicted\n",
    "    feature_scaler (StandardScaler): Scaler used for features\n",
    "    target_scaler (StandardScaler): Scaler used for target\n",
    "      \n",
    "    Returns: \n",
    "    df_scaled (pd.DataFrame): Scaled dataframe   \n",
    "    \"\"\"    \n",
    "    \n",
    "    target = df[target_col].values.reshape(-1, 1)\n",
    "    if fit:\n",
    "        target_scaler.fit(target)\n",
    "    target_scaled = target_scaler.transform(target)\n",
    "    \n",
    "    # Select all columns other than target to be features\n",
    "    features = df.loc[:, df.columns != target_col].values\n",
    "    \n",
    "    if features.shape[1]:  # If there are any features\n",
    "        if fit:\n",
    "            feature_scaler.fit(features)\n",
    "        features_scaled = feature_scaler.transform(features)\n",
    "        \n",
    "        # Combine target and features into one data frame\n",
    "        df_scaled = pd.DataFrame(features_scaled)\n",
    "        target_col_num = df.columns.get_loc(target_col)\n",
    "        df_scaled.insert(target_col_num, target_col, target_scaled)\n",
    "        df_scaled.columns = df.columns        \n",
    "    \n",
    "    else:  # If only target column (no additional features)\n",
    "        df_scaled = pd.DataFrame(target_scaled, columns=df.columns)\n",
    "      \n",
    "    return df_scaled\n",
    "\n",
    "def inverse_scale(data, target_scaler=target_scaler):\n",
    "    \"\"\"\n",
    "    Transform the scaled values of the target back into their original form.\n",
    "    The features are left alone, as we're assuming that the output of the model only includes the target.\n",
    "    \n",
    "    Parameters: \n",
    "    data (np.array): Input array\n",
    "    target_scaler (StandardScaler): Scaler used for target\n",
    "      \n",
    "    Returns: \n",
    "    data_scaled (np.array): Scaled array   \n",
    "    \"\"\"    \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    data_scaled = np.empty([data.shape[1], data.shape[0]])\n",
    "    for i in range(data.shape[1]):\n",
    "        data_scaled[i] = target_scaler.inverse_transform([data[:,i]])\n",
    "    return data_scaled.transpose()\n",
    "\n",
    "df_train_scaled=scale(df_train)\n",
    "df_test_scaled=scale(df_test, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba206b9-46df-43cc-90ae-ea2f9b15a421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sequences of time series data\n",
    "def reframe(data, n_input_steps = n_input_steps, n_output_steps = n_output_steps, target_col = target_col):\n",
    "\n",
    "    target_col_num = data.columns.get_loc(target_col)    \n",
    "    \n",
    "    # Iterate through data and create sequences of features and outputs\n",
    "    df = pd.DataFrame(data)\n",
    "    cols=list()\n",
    "    for i in range(n_input_steps, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "    for i in range(0, n_output_steps):\n",
    "        cols.append(df.shift(-i))\n",
    "        \n",
    "    # Concatenate values and remove any missing values\n",
    "    df = pd.concat(cols, axis=1)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Split the data into feature and target variables\n",
    "    n_feature_cols = n_input_steps * n_features\n",
    "    features = df.iloc[:,0:n_feature_cols]\n",
    "    target_cols = [i for i in range(n_feature_cols + target_col_num, n_feature_cols + n_output_steps * n_features, n_features)]\n",
    "    targets = df.iloc[:,target_cols]\n",
    "\n",
    "    return (features, targets)\n",
    "\n",
    "X_train_reframed, y_train_reframed = reframe(df_train_scaled)\n",
    "X_test_reframed, y_test_reframed = reframe(df_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a857df-d87d-445a-9e8d-96da5a99a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval results\n",
    "def print_stats(timestep, y_true, y_pred, target_col, chart=True, table=False, dec=3):\n",
    "    '''\n",
    "    Helper function to print overall summary statistics and stats for each time step\n",
    "    '''\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print('=== t+' + str(timestep) + ' ===')\n",
    "    print('R^2:  ' + str(np.round(r2_score(y_true, y_pred), dec)))\n",
    "    print('MAPE: ' + str(np.round(mean_absolute_percentage_error(y_true, y_pred), dec)))\n",
    "    print('MAE:  ' + str(np.round(mean_absolute_error(y_true, y_pred), dec)))\n",
    "    print('')\n",
    "\n",
    "    df_y_true = pd.DataFrame(y_true)\n",
    "    df_y_true[target_col + '_pred'] = np.round(y_pred, dec)\n",
    "    \n",
    "    # Show plot of actuals vs predictions and a sample of values\n",
    "    if table:\n",
    "        print(str(df_y_true.head(5)) + '\\n')\n",
    "        print(str(df_y_true.tail(5)) + '\\n')\n",
    "    if chart:\n",
    "        sns.lineplot(data=df_y_true[[target_col, target_col+'_pred']])\n",
    "        plt.show()\n",
    "        \n",
    "def evaluate(y_pred,\n",
    "             exclude_timesteps=n_input_steps,\n",
    "             y_true=df_test,\n",
    "             target_col=target_col):\n",
    "    '''\n",
    "    Helper function to transform predictions to match size and indices of actuals.\n",
    "    \n",
    "    For example, n_timesteps from the test data will be required to make a prediction,\n",
    "    so the number of predictions will be fewer than the number of test samples.\n",
    "    \n",
    "    Parameters:\n",
    "    y_pred (np.array): Predictions\n",
    "    exclude_timesteps (int): Number of leading timesteps to trim from the dataset\n",
    "    y_true (pd.DataFrame): Actuals\n",
    "    '''\n",
    "        \n",
    "    # Number of outputs (future timesteps)\n",
    "    outputs = y_pred.shape[1]\n",
    "    \n",
    "    target_col_num = df.columns.get_loc(target_col)\n",
    "    \n",
    "    # Lists of actual and predicted values for each time step\n",
    "    # For example, y_true_eval[2] will contain actual values 3 time steps out\n",
    "    # These specific lists enable computing the accuracy for specific time steps\n",
    "    y_true_eval, y_pred_eval = list(), list()\n",
    "\n",
    "    # Actual and predicted values combined across all time steps (to compute overall accuracy metrics)\n",
    "    y_true_all, y_pred_all = np.array([]), np.array([])\n",
    "    \n",
    "    # Append entries to lists for each output timestep\n",
    "    for t in range(outputs):\n",
    "        if exclude_timesteps:\n",
    "            y_true_eval.append(y_true[exclude_timesteps+t:len(y_true)-outputs+t+1].copy())\n",
    "            y_pred_eval.append(y_pred[:,t])          \n",
    "        else:\n",
    "            y_true_eval.append(y_true[t:].copy())\n",
    "            y_pred_eval.append(y_pred[:-1*t-1,t])\n",
    "        # Append the output values to the combined lists\n",
    "        y_true_all = np.concatenate([y_true_all, y_true_eval[t].values[:,target_col_num]], axis=0)\n",
    "        y_pred_all = np.concatenate([y_pred_all, y_pred_eval[t]], axis=0)\n",
    "\n",
    "    # Print aggregate statistics across all time steps (only if predicting multiple time steps)\n",
    "    if outputs > 1:\n",
    "        print_stats('(1-' + str(outputs) + ')', y_true_all, y_pred_all, target_col, False)\n",
    "\n",
    "    # Print stats for each future time step\n",
    "    for t in range(outputs):    \n",
    "        print_stats(t+1, y_true_eval[t][target_col], y_pred_eval[t], target_col, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72237fc-7a53-41b3-ba24-48793965afdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train models - in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c74e0f-8114-4c7d-9a46-82fe7f84a6c2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b082a-8aea-4018-b7a9-0f0253695b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape test data to match model inputs and outputs, necessary for neural networks\n",
    "\n",
    "X_train = X_train_reframed.values.reshape(-1, n_input_steps, n_features)\n",
    "X_test = X_test_reframed.values.reshape(-1, n_input_steps, n_features)\n",
    "y_train = y_train_reframed.values.reshape(-1, n_output_steps, 1)\n",
    "y_test = y_test_reframed.values.reshape(-1, n_output_steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a62c5d-9d9e-411e-b534-f342bcba637b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Try increasing and decreasing the number of LSTM units and see if you notice any accuracy improvements.\n",
    "# Run the next cell to evaluate the results in more detail.\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=[n_input_steps, n_features]),\n",
    "    Dense(n_output_steps)])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "_ = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549d5f7-cbd8-4e06-b222-a9766ee033cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./lstm_export/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a00d2-0fe9-4dba-8954-4c3d23edd35a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Predict the results, and then reverse the transformation that scaled all values to a mean of 0 and std. dev. of 1\n",
    "preds = model.predict(X_test)\n",
    "y_pred_lstm = inverse_scale(preds)\n",
    "y_pred_lstm\n",
    "# Evaluate the overall results and for each time step\n",
    "#evaluate(y_pred_lstm)\n",
    "\n",
    "# The plot will show the R^2 value (0 lowest -> 1 highest) and the MAE (mean absolute error) for the entire prediction window.\n",
    "# It will also show individual plots for 1 day out, 2 days out, etc. comparing the actual vs the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab409ba5-721a-42e6-8213-fe958c8c717d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2e126-68df-4624-a560-807e1fd97cfc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.keras.layers import AveragePooling1D\n",
    "\n",
    "# Try adjusting the # of filters (pattern types) and kernel size (size of the sliding window)\n",
    "model = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, input_shape=[n_input_steps, n_features]),\n",
    "    Flatten(),\n",
    "    Dense(n_output_steps)])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "_ = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e337dc01-d9f4-4f77-828a-3982fc894064",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./cnn_export/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d049cd0-374a-4b56-856d-b3ad5b2edf2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "preds = model.predict(X_test)\n",
    "y_pred_cnn = inverse_scale(preds)\n",
    "\n",
    "evaluate(y_pred_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95389ac9-7f6b-4813-a049-8979bf158a2b",
   "metadata": {},
   "source": [
    "## Naïve Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dc12ad-140f-4083-bdf8-22cf673ff11d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2440f2-5a70-4abe-aaab-23e3df2e1b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "hist = df_train[target_col].copy() # Predict based on historical data. Start with the training data\n",
    "hist.index.freq = pd.infer_freq(hist.index) # To avoid warnings, explicitly specify the dataframe frequency\n",
    "n_pred = len(df_test) + 1 # Number of predictions: 1 on the training set; and then 1 for each additional \n",
    "y_pred_rw = np.empty([n_pred,n_output_steps]) # Create an array to hold predictions, with a number of predictions equal to the test set size, each containing the # of time steps you are predicting forward.\n",
    "\n",
    "for t in range(n_pred):\n",
    "    mod = ARIMA(hist, order=(0, 1, 0))\n",
    "    res = mod.fit()\n",
    "    pred = res.forecast(n_output_steps)\n",
    "    y_pred_rw[t] = pred.values\n",
    "    if t < n_pred - 1:\n",
    "        hist.loc[df_test.iloc[t].name] = df_test[target_col][t] # Append the latest test data row to the history, for fitting the next model\n",
    "        hist.index.freq = pd.infer_freq(hist.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbb442-a5c3-47d6-ad4c-35f655a11584",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(y_pred_rw, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15230276-2eec-4237-8a0b-ae1ebfe1cde5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Seasonal Naïve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbcb1d-90b4-47e7-9441-0dfa38d0f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# You will use a walk-forward approach, in which a model is fit on all historical data available.\n",
    "# As you progress through the test set to evaluate the model, you will be creating new models for each row in the test set.\n",
    "# Each new model will be fit on not only the training data, but on prior test data.\n",
    "\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "hist = df_train[target_col].copy() # Predict based on historical data. Start with the training data\n",
    "hist.index.freq = pd.infer_freq(hist.index) # To avoid warnings, explicitly specify the dataframe frequency\n",
    "n_pred = len(df_test) + 1 # Number of predictions: 1 on the training set; and then 1 for each additional \n",
    "y_pred_sn = np.empty([n_pred,n_output_steps]) # Create an array to hold predictions, with a number of predictions equal to the test set size, each containing the # of time steps you are predicting forward.\n",
    "\n",
    "for t in range(n_pred):\n",
    "    mod = SARIMAX(hist, order=(0, 0, 0), seasonal_order=(0, 1, 0, n_seasons))\n",
    "    res = mod.fit(disp=False)\n",
    "    pred = res.forecast(n_output_steps)\n",
    "    y_pred_sn[t] = pred.values\n",
    "    if t < n_pred - 1:\n",
    "        hist.loc[df_test.iloc[t].name] = df_test[target_col][t] # Append the latest test data row to the history, for fitting the next model\n",
    "        hist.index.freq = pd.infer_freq(hist.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279f1431-1609-4c7d-b8ca-fbf7203615b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate(y_pred_sn, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea92c0-2d46-4c20-a374-917211dc0957",
   "metadata": {},
   "source": [
    "## Statistical Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cb21b2-396a-413f-95e3-ed9864fb42b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ef7d1-122b-401a-8a55-4dafae1a96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82b85b-5ef7-432e-81c1-12b1e762e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# You will use a walk-forward approach, in which a model is fit on all historical data available.\n",
    "# As you progress through the test set to evaluate the model, you will be creating new models for each row in the test set.\n",
    "# Each new model will be fit on not only the training data, but on prior test data.\n",
    "\n",
    "hist = df_train[target_col].copy() # Predict based on historical data. Start with the training data\n",
    "hist.index.freq = pd.infer_freq(hist.index) # To avoid warnings, explicitly specify the dataframe frequency\n",
    "n_pred = len(df_test) + 1 # Number of predictions: 1 on the training set; and then 1 for each additional \n",
    "y_pred_es = np.empty([n_pred,n_output_steps]) # Create an array to hold predictions, with a number of predictions equal to the test set size, each containing the # of time steps you are predicting forward.\n",
    "\n",
    "for t in range(n_pred):\n",
    "    mod = ExponentialSmoothing(hist, seasonal_periods=n_seasons, trend='add', seasonal='add', damped_trend=True, use_boxcox=False, initialization_method='heuristic')\n",
    "    res = mod.fit(method='L-BFGS-B')  # Use a different minimizer to avoid convergence warnings\n",
    "    pred = res.forecast(n_output_steps)\n",
    "    y_pred_es[t] = pred.values\n",
    "    if t < n_pred - 1:\n",
    "        hist.loc[df_test.iloc[t].name] = df_test[target_col][t] # Append the latest test data row to the history, for fitting the next model\n",
    "        hist.index.freq = pd.infer_freq(hist.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241870a-22bf-4a94-9fa4-1116a6234af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(y_pred_es, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0cf4d1-bc0b-4c11-a167-c7ddc4765088",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "###  Ensemble ML and Statistical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551dc88-1e71-4b7f-a944-b64bd42b8ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Start by adjusting the sizes of the prediction arrays to match.\n",
    "# Some methods predict the initial timesteps of the test set.\n",
    "# Others start after the first sequence length.\n",
    "# So, you will remove the test data that doesn't exist in both sets.\n",
    "\n",
    "def trunc(df, test_set=df_test, n_input_steps = n_input_steps, n_output_steps = n_output_steps):\n",
    "    return df[n_input_steps: -n_output_steps]\n",
    "\n",
    "y_pred_es_trunc = trunc(y_pred_es)\n",
    "y_true_trunc = trunc(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b741cc7-bc7d-491a-93df-146417b8ff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "models = [y_pred_lstm, y_pred_cnn, y_pred_es_trunc]\n",
    "weights = [2, 1, 1]\n",
    "\n",
    "y_pred_ensemble = np.average( np.array(models), axis=0, weights=weights)\n",
    "\n",
    "evaluate(y_pred_ensemble, 0, y_true_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0b7fea-00d9-4fe9-930a-8ed4ceb52e4f",
   "metadata": {},
   "source": [
    "# Train models - in Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2aee4-2de6-4ba1-b19a-85586fe4b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import aiplatform, storage\n",
    "from google.cloud.aiplatform import gapic as aip\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11b5cd-6e9a-434c-b896-42632395b929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the TensorFlow version installed\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84cd553-0198-4fe3-9cac-c01cef1ce176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Vertex SDK for Python\n",
    "\n",
    "aiplatform.init(project=PROJECT, location=REGION, staging_bucket=BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ebc349-ff68-494c-83c3-93a3f5859f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters\n",
    "\n",
    "target_col = 'total_rides' # The variable you are predicting\n",
    "ts_col = 'service_date' # The name of the column with the date field\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "freq = 'D' # Daily frequency\n",
    "n_input_steps = 30 # Lookback window\n",
    "n_output_steps = 7 # How many steps to predict forward\n",
    "n_seasons = 7 # Monthly periodicity\n",
    "\n",
    "train_split = 0.8 # % Split between train/test data\n",
    "epochs = 1000 # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = 5 # Terminate training after the validation loss does not decrease after this many epochs\n",
    "\n",
    "lstm_units = 64\n",
    "input_layer_name = 'lstm_input'\n",
    "\n",
    "# Training parameters\n",
    "\n",
    "MODEL_NAME = 'cta_ridership'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741e181-41a2-4592-ac94-433929f66d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories to be used later\n",
    "\n",
    "TRAINER_DIR = 'trainer'\n",
    "EXPORT_DIR = 'tf_export'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54760f4-807a-4952-8aab-87c9bed9f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer directory if it doesn't already exist\n",
    "\n",
    "!mkdir $TRAINER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a24d6a-2e1c-408f-8ca0-12c4d828bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy numpy arrays to npy files\n",
    "\n",
    "np.save(TRAINER_DIR + '/x_train.npy', X_train)\n",
    "np.save(TRAINER_DIR + '/x_test.npy', X_test)\n",
    "np.save(TRAINER_DIR + '/y_train.npy', y_train)\n",
    "np.save(TRAINER_DIR + '/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cbaa8a-e6ba-41e5-a77c-6f8e775712fb",
   "metadata": {},
   "source": [
    "## Prepare model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587aef89-271a-4b0d-9fca-1cc80cd0e3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write training code out to a file that will be submitted to the training job\n",
    "# Note: f-strings are supported in Python 3.6 and above\n",
    "\n",
    "model_template = f\"\"\"import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from google.cloud import storage\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "n_features = {n_features} # Two features: y (previous values) and whether the date is a holiday\n",
    "n_input_steps = {n_input_steps} # Lookback window\n",
    "n_output_steps = {n_output_steps} # How many steps to predict forward\n",
    "\n",
    "epochs = {epochs} # How many passes through the data (early-stopping will cause training to stop before this)\n",
    "patience = {patience} # Terminate training after the validation loss does not decrease after this many epochs\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    '''Downloads a blob from the bucket.'''\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print(\"Blob \" + source_blob_name + \" downloaded to \" + destination_file_name + \".\")\n",
    "\n",
    "def extract_bucket_and_prefix_from_gcs_path(gcs_path: str):\n",
    "    '''Given a complete GCS path, return the bucket name and prefix as a tuple.\n",
    "\n",
    "    Example Usage:\n",
    "\n",
    "        bucket, prefix = extract_bucket_and_prefix_from_gcs_path(\n",
    "            \"gs://example-bucket/path/to/folder\"\n",
    "        )\n",
    "\n",
    "        # bucket = \"example-bucket\"\n",
    "        # prefix = \"path/to/folder\"\n",
    "\n",
    "    Args:\n",
    "        gcs_path (str):\n",
    "            Required. A full path to a Google Cloud Storage folder or resource.\n",
    "            Can optionally include \"gs://\" prefix or end in a trailing slash \"/\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, Optional[str]]\n",
    "            A (bucket, prefix) pair from provided GCS path. If a prefix is not\n",
    "            present, a None will be returned in its place.\n",
    "    '''\n",
    "    if gcs_path.startswith(\"gs://\"):\n",
    "        gcs_path = gcs_path[5:]\n",
    "    if gcs_path.endswith(\"/\"):\n",
    "        gcs_path = gcs_path[:-1]\n",
    "\n",
    "    gcs_parts = gcs_path.split(\"/\", 1)\n",
    "    gcs_bucket = gcs_parts[0]\n",
    "    gcs_blob_prefix = None if len(gcs_parts) == 1 else gcs_parts[1]\n",
    "\n",
    "    return (gcs_bucket, gcs_blob_prefix)\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--data-uri',\n",
    "        default=None,\n",
    "        help='URL where the training files are located')\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "def main():\n",
    "    args = get_args()\n",
    "    bucket_name, blob_prefix = extract_bucket_and_prefix_from_gcs_path(args.data_uri)\n",
    "    \n",
    "    # Get the training data and convert back to np arrays\n",
    "    local_data_dir = os.path.join(os.getcwd(), tempfile.gettempdir())\n",
    "    files = ['x_train.npy', 'y_train.npy', 'x_test.npy', 'y_test.npy']\n",
    " \n",
    "    for file in files:\n",
    "        download_blob(bucket_name, os.path.join(blob_prefix,file), os.path.join(local_data_dir,file))\n",
    "\n",
    "    X_train = np.load(local_data_dir + '/x_train.npy')\n",
    "    y_train = np.load(local_data_dir + '/y_train.npy')\n",
    "    X_test = np.load(local_data_dir + '/x_test.npy')\n",
    "    y_test = np.load(local_data_dir + '/y_test.npy')\n",
    "        \n",
    "    # Build and train the model\n",
    "    model = Sequential([\n",
    "        LSTM({lstm_units}, input_shape=[n_input_steps, n_features], recurrent_activation=None),\n",
    "        Dense(n_output_steps)])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    _ = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[early_stopping])\n",
    "    \n",
    "    # Export the model\n",
    "    model.save(os.environ[\"AIP_MODEL_DIR\"])\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(TRAINER_DIR, 'task.py'), 'w') as f:\n",
    "    f.write(model_template.format(**globals()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32915ed-53b8-467f-b64c-159dcaf0497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data files to a GCS bucket\n",
    "\n",
    "!gsutil -m cp -r trainer/*.npy $BUCKET_URI/$TRAINER_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3b2e1-868b-4807-8ea8-694394dcf5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the contents of the bucket to ensure they were copied properly\n",
    "\n",
    "!gsutil ls $BUCKET_URI/$TRAINER_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8304c-7be7-4b50-b90c-fb7e729e2e7d",
   "metadata": {},
   "source": [
    "## Submit training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e2326-9905-49ce-8a63-e877b3764db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training job parameters\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--data-uri={BUCKET_URI}/{TRAINER_DIR}\"\n",
    "]\n",
    "TRAIN_VERSION = \"tf-cpu.2-6\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-6\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)\n",
    "\n",
    "# Re-run these additional parameters if you need to create a new training job\n",
    "\n",
    "TIMESTAMP = str(datetime.datetime.now().time())\n",
    "JOB_NAME = 'vertex_ai_training_' + TIMESTAMP\n",
    "MODEL_DISPLAY_NAME = MODEL_NAME + TIMESTAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a8090-9039-4121-829c-5e4e8bc897c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and run the training job\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=f\"{TRAINER_DIR}/task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "model = job.run(\n",
    "        model_display_name=MODEL_DISPLAY_NAME,\n",
    "        args=CMDARGS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44292f-2ef1-40f5-9a19-ee70808510c2",
   "metadata": {},
   "source": [
    "## Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f913362-e1fd-470f-8ccc-9f8bea022a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEPLOYED_NAME = f\"{MODEL_NAME}_deployed-\" + TIMESTAMP\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    "    traffic_split={\"0\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fcf5db-6b07-4711-bc50-cf5d0b776442",
   "metadata": {},
   "source": [
    "## Get predictions on deployed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58b7fa5-d796-43a2-a341-777989592da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the first test instance\n",
    "\n",
    "raw_predictions = endpoint.predict(instances=X_test.tolist()).predictions[0]\n",
    "predicted_values = inverse_scale(np.array([raw_predictions])).round()\n",
    "\n",
    "actual_values = inverse_scale(np.array([y_test[0]]))\n",
    "\n",
    "# Print prediction and compare to actual value\n",
    "\n",
    "print('Predicted riders:', predicted_values)\n",
    "print('Actual riders:   ', actual_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d51eab-3792-474d-896c-5c1e10802c47",
   "metadata": {},
   "source": [
    "# Cleanup - if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11cd72-7181-4022-8615-f9b159f4dfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_training_job = True\n",
    "delete_model = True\n",
    "delete_endpoint = True\n",
    "\n",
    "# Warning: Setting this to true will delete everything in your bucket\n",
    "delete_bucket = False\n",
    "\n",
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the endpoint\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "# Warning: uncomment this section only if you want to delete the entire bucket\n",
    "# if delete_bucket and \"BUCKET\" in globals():\n",
    "#     ! gsutil -m rm -r $BUCKET"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
