{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ef77e0-4160-451e-90cf-2d415d7634a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import argparse\n",
    "import apache_beam as beam\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import StandardOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText\n",
    "\n",
    "import google.auth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8af55b-b72e-437c-8fa0-e4886d2a2a97",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataflow Batch - GCS Text Files to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b75dd2-0d94-475a-a755-de4b089afc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_batch(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "\n",
    "    SCHEMA = 'eventId:STRING,deviceId:STRING,eventTime:DATETIME,city:STRING,temp:FLOAT,flowrate:FLOAT'\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner',           required=False, default='DataflowRunner',            help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name',         required=False, default='mgwaterdemobatch_'+str(dt_string),             help='Dataflow Job Name')\n",
    "    parser.add_argument('--project_id',       required=False, default='mg-ce-demos',             help='GCP Project ID')\n",
    "    parser.add_argument('--region',           required=False, default='us-central1',             help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name',     required=False, default='smart_water_demo',        help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name',       required=False, default='smart_water_demo_data_'+str(dt_string),   help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data',       required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='GCS Bucket and file suffix for input data')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          # CHANGE 2/7: (OPTIONAL) Change this to DataflowRunner to\n",
    "          # run your pipeline on the Google Cloud Dataflow Service. DirectRunner (local)\n",
    "          #'--runner=DirectRunner',\n",
    "          '--runner=' + str(known_args.runner)\n",
    "          # CHANGE 3/7: Your project ID is required in order to run your pipeline on\n",
    "          # the Google Cloud Dataflow Service.\n",
    "          #'--project=mg-ce-demos',\n",
    "          ,'--project=' + str(known_args.project_id)\n",
    "          # CHANGE 4/7: Your Google Cloud Storage path is required for staging local\n",
    "          # files.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp'\n",
    "          # CHANGE 5/7: Your Google Cloud Storage path is required for temporary\n",
    "          # files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp'\n",
    "          # CHANGE 6/7: '--job_name=mgwaterdemo1'\n",
    "          ,'--job_name=' + str(known_args.job_name) + str(dt_string)\n",
    "          # CHANGE 7/7: (OPTIONAL) Set region if using DataflowRunner\n",
    "          ,'--region=' + str(known_args.region)\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    #pipeline_options.view_as(StandardOptions).streaming = True  #remove if batch\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the JSON files in GCS into a PCollection.\n",
    "        events = ( p | beam.io.ReadFromText(known_args.input_data) )  #change to read files from GCS\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b1050c-f84e-45af-95fd-778d5ff64267",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805dcd4f-c6bc-4666-a7a2-b98adfe6f5d2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Dataflow Stream - Pub/Sub Topic to BQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a6fb4-fd0f-48b7-8974-5ecde0100b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# function(s)\n",
    "\n",
    "def parse_json(line):\n",
    "    import json\n",
    "    record = json.loads(line)\n",
    "    \n",
    "    return record\n",
    "\n",
    "def run_stream(argv=None):\n",
    "    \"\"\"Build and run the pipeline.\"\"\"\n",
    "    from datetime import datetime\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd_mm_YY_H_M_S_ssssss\n",
    "    dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "    dt_string\n",
    "    \n",
    "    SCHEMA = 'eventId:STRING,deviceId:STRING,eventTime:DATETIME,city:STRING,temp:FLOAT,flowrate:FLOAT'  # Simple BQ Schema\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--runner',           required=False, default='DataflowRunner',          help='Dataflow Runner - DataflowRunner or DirectRunner (local)')\n",
    "    parser.add_argument('--job_name',         required=False, default='mgwaterdemostream_'+str(dt_string),            help='Dataflow Job Name')\n",
    "    parser.add_argument('--batch_size',       required=False, default='100',                     help='Dataflow Batch Size')\n",
    "    parser.add_argument('--input_topic',      required=False, default='projects/mg-ce-demos/topics/smart-water',             help='Input PubSub Topic: projects/<project_id>/topics/<topic_name>')\n",
    "    parser.add_argument('--project_id',       required=False, default='mg-ce-demos',             help='GCP Project ID')\n",
    "    parser.add_argument('--region',           required=False, default='us-central1',             help='GCP region for execution')\n",
    "    parser.add_argument('--dataset_name',     required=False, default='smart_water_demo',        help='Output BigQuery Dataset') \n",
    "    parser.add_argument('--table_name',       required=False, default='smart_water_demo_data_stream_'+str(dt_string),   help='Output BigQuery Table')\n",
    "    parser.add_argument('--input_data',       required=False, default='gs://mg-ce-demos-bucket/water_data_stream_demo/output/*.json', help='GCS Bucket and file suffix for input data')\n",
    "\n",
    "    \n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "    \n",
    "    pipeline_args.extend([\n",
    "          # CHANGE 2/7: (OPTIONAL) Change this to DataflowRunner to\n",
    "          # run your pipeline on the Google Cloud Dataflow Service. DirectRunner (local)\n",
    "          #'--runner=DirectRunner',\n",
    "          '--runner=' + str(known_args.runner)\n",
    "          # CHANGE 3/7: Your project ID is required in order to run your pipeline on\n",
    "          # the Google Cloud Dataflow Service.\n",
    "          #'--project=mg-ce-demos',\n",
    "          ,'--project=' + str(known_args.project_id)\n",
    "          # CHANGE 4/7: Your Google Cloud Storage path is required for staging local\n",
    "          # files.\n",
    "          ,'--staging_location=gs://mg-ce-demos-bucket/water_data_stream_demo/temp'\n",
    "          # CHANGE 5/7: Your Google Cloud Storage path is required for temporary\n",
    "          # files.\n",
    "          ,'--temp_location=gs://mg-ce-demos-bucket/water_data_stream_demo/tmp'\n",
    "          # CHANGE 6/7: '--job_name=mgwaterdemo1'\n",
    "          ,'--job_name=' + str(known_args.job_name)\n",
    "          # CHANGE 7/7: (OPTIONAL) Set region if using DataflowRunner\n",
    "          ,'--region=' + str(known_args.region)\n",
    "      ])\n",
    "    \n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "    pipeline_options.view_as(StandardOptions).streaming = True  #remove if batch\n",
    "    \n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        \n",
    "        # Read the pubsub topic into a PCollection.\n",
    "        events = ( p | beam.io.ReadStringsFromPubSub(known_args.input_topic) )\n",
    "\n",
    "        # Tranform events\n",
    "        transformed = (events | beam.Map(parse_json))\n",
    "\n",
    "        # Persist to BigQuery\n",
    "        transformed | 'Write' >> beam.io.WriteToBigQuery(\n",
    "                            table=known_args.table_name,\n",
    "                            dataset=known_args.dataset_name,\n",
    "                            project=known_args.project_id,\n",
    "                            schema=SCHEMA,\n",
    "                            create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED,\n",
    "                            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n",
    "                            batch_size=int(known_args.batch_size)\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921ba584-d3cc-4958-b8c3-f3ddd95b331e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#Execute\n",
    "if __name__ == '__main__':\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    run_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327844ae-6855-49a0-88d8-1c5af382aa5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca9cdab-1e74-4341-8aea-d6e6ec5d5e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to do BQ schema - more details\n",
    "'''\n",
    "    table_schema = bigquery.TableSchema()\n",
    "    \n",
    "    # Fields that use standard types.\n",
    "    eventId_schema = bigquery.TableFieldSchema()\n",
    "    eventId_schema.name = 'eventId'\n",
    "    eventId_schema.type = 'string'\n",
    "    eventId_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(eventId_schema)\n",
    "    \n",
    "    deviceId_schema = bigquery.TableFieldSchema()\n",
    "    deviceId_schema.name = 'deviceId'\n",
    "    deviceId_schema.type = 'string'\n",
    "    deviceId_schema.mode = 'required'\n",
    "    table_schema.fields.append(deviceId_schema)\n",
    "    \n",
    "    eventTime = bigquery.TableFieldSchema()\n",
    "    eventTime.name = 'eventTime'\n",
    "    eventTime.type = 'datetime'\n",
    "    eventTime.mode = 'nullable'\n",
    "    table_schema.fields.append(eventTime)\n",
    "    \n",
    "    city_schema = bigquery.TableFieldSchema()\n",
    "    city_schema.name = 'city'\n",
    "    city_schema.type = 'string'\n",
    "    city_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(city_schema)\n",
    "    \n",
    "    temp_schema = bigquery.TableFieldSchema()\n",
    "    temp_schema.name = 'temp'\n",
    "    temp_schema.type = 'float'\n",
    "    temp_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(temp_schema)\n",
    "    \n",
    "    flowrate_schema = bigquery.TableFieldSchema()\n",
    "    flowrate_schema.name = 'flowrate'\n",
    "    flowrate_schema.type = 'float'\n",
    "    flowrate_schema.mode = 'nullable'\n",
    "    table_schema.fields.append(flowrate_schema)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fcdd33-7633-4f08-8be1-64bdedea81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "# dd_mm_YY_H_M_S_ssssss\n",
    "dt_string = now.strftime(\"%m_%d_%Y_%H_%M_%S_%f\")\n",
    "dt_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d60044-bf45-4f65-b65e-427facbd8b65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "01. Apache Beam 2.37.0 for Python 3",
   "language": "python",
   "name": "01-apache-beam-2.37.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
